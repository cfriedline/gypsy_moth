{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import scandir\n",
    "import os\n",
    "import rpy2\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "import rpy2.robjects as ro\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import dill\n",
    "import random\n",
    "import vcf\n",
    "from hdfstorehelper import HDFStoreHelper\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import operator\n",
    "import traceback\n",
    "%load_ext rpy2.ipython\n",
    "from rpy2.robjects import pandas2ri as p2r\n",
    "p2r.activate()\n",
    "r = ro.r\n",
    "import shutil\n",
    "from utils import read_df, save_df\n",
    "from pathlib import Path, PurePath\n",
    "from ipyparallel import Client\n",
    "from collections import Counter, defaultdict, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analysis_dir_notimp = \"/home/cfriedline/eckertlab/gypsy_indiv/raw_demult/analysis/samtools1.3_masurca3/ni/\"\n",
    "analysis_dir_imp = \"/home/cfriedline/eckertlab/gypsy_indiv/raw_demult/analysis/samtools1.3_masurca3/beagle40/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analysis_dir = [analysis_dir_notimp, analysis_dir_imp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gps = {'QC32':[47.2509807, -79.4060515],\n",
    "      'QC93': [46.9089631, -70.8061075],\n",
    "      'NC': [36.449125, -76.024672],\n",
    "      'NY': [42.897768, -74.094761],\n",
    "      'VA1': [38.657615, -77.463603],\n",
    "      'VA2': [38.857470, -77.695003]}\n",
    "gps_df = pd.DataFrame(gps).T\n",
    "gps_df.columns = ['lat','lon']\n",
    "\n",
    "latlon = pandas2ri.py2ri(gps_df[['lon', 'lat']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(sp)\n",
    "library(raster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raster = r(\"raster\")\n",
    "extract = r(\"extract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bioclim_dir = \"/home/cfriedline/eckertlab/bioclim\"\n",
    "bioclim = !ls {bioclim_dir}/*.bil\n",
    "bioclim = sorted(bioclim)\n",
    "bioclim_df = pd.DataFrame(gps_df)\n",
    "for b in bioclim:\n",
    "    rast = raster(b)\n",
    "    bio = os.path.basename(b).replace(\".bil\", \"\").replace(\"_\", \"\").upper()\n",
    "    vals = pd.DataFrame(pandas2ri.ri2py(extract(rast, latlon)))\n",
    "    vals.index = bioclim_df.index\n",
    "    vals.columns = [bio]\n",
    "    bioclim_df = bioclim_df.join(vals)\n",
    "bioclim_df = bioclim_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for d in analysis_dir:\n",
    "    save_df(d, \"bioclim_df\", bioclim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "region_id = 1\n",
    "for popname in bioclim_df.index:\n",
    "    bioclim_df.ix[popname, 'region'] = region_id\n",
    "    region_id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bioclim_env_files = []\n",
    "for bio in bioclim_df[[x for x in bioclim_df if 'BIO' in x]]:\n",
    "    bio_temp = bioclim_df[[bio, 'region']]\n",
    "    bio_temp.index.name = \"CLST\"\n",
    "    bio_temp.columns = [\"ENV\", \"REG\"]\n",
    "    bio_out = os.path.join(analysis_dir[0], \"../%s.txt\" % bio)\n",
    "    bioclim_env_files.append(bio_out)\n",
    "    bio_temp.to_csv(bio_out, sep=\"\\t\", header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z12_swapped = [read_df(x, 'z12_swapped') for x in analysis_dir]\n",
    "z12 = [read_df(x, 'z12_df') for x in analysis_dir]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def get_correction(n):\n",
    "    #for finite sample size\n",
    "    return (2*n)/(2*n-1)\n",
    "\n",
    "def get_allele_freqs(locus, debug):\n",
    "    import pandas as pd\n",
    "    try:\n",
    "        c = locus[locus != -1].value_counts()\n",
    "        if not len(c):\n",
    "            return pd.Series()\n",
    "        total_alleles = 2.0*sum(c)    \n",
    "        num_individuals = sum(c)\n",
    "        P = 0\n",
    "        Q = 0\n",
    "        PQ = 0\n",
    "        if 0 in c:\n",
    "            P = 2*c[0]\n",
    "        if 2 in c:\n",
    "            Q = 2*c[2]\n",
    "        if 1 in c:\n",
    "            PQ = c[1]\n",
    "        P += PQ\n",
    "        Q += PQ\n",
    "        p = P/total_alleles\n",
    "        q = Q/total_alleles\n",
    "        assert p + q == 1.0\n",
    "        He = 2 * p * q * get_correction(num_individuals)\n",
    "        Ho = PQ*1.0/num_individuals\n",
    "        Fis = 1 - (Ho/He)\n",
    "        #print p, q, He, Ho, Fis\n",
    "\n",
    "\n",
    "        ret = pd.Series({\"p\":p, \n",
    "                          \"q\":q,\n",
    "                          \"P\":P,\n",
    "                          \"Q\":Q,\n",
    "                          \"He\":He,\n",
    "                          \"Ho\":Ho, \n",
    "                          \"Fis\":Fis})\n",
    "        if debug:\n",
    "            print(ret)\n",
    "        return ret\n",
    "    except:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allele_freqs = [x.ix[:,:-2].apply(get_allele_freqs, args=(False,)) for x in z12_swapped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allele_freqs[0].shape, allele_freqs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, af in enumerate(allele_freqs):\n",
    "    save_df(analysis_dir[i], 'allele_freqs_squat', af)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gt_base_df = [read_df(x, 'gt_base_df') for x in analysis_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = gt_base_df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pops = sorted(set([x.split(\"_\")[0] for x in df.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_genotypes(snp):\n",
    "    counts = Counter()\n",
    "    for gt in snp:\n",
    "        try:\n",
    "            float(gt) #if gt is nan\n",
    "        except:\n",
    "            counts[gt[0]]+=1\n",
    "            counts[gt[-1]]+=1\n",
    "    return sorted(counts.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_af_to_count(data):\n",
    "    total = sum([x[1] for x in data])\n",
    "    ret = []\n",
    "    for elem in data:\n",
    "        elem = list(elem)[0:2]\n",
    "        elem.append(elem[1]/total)\n",
    "        ret.append(elem)\n",
    "    if len(ret) == 1:\n",
    "        ret.append()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gt_counts = df.apply(count_genotypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SNP = namedtuple(\"SNP\", ['name',\n",
    "                             'minor_allele',\n",
    "                             'minor_count',\n",
    "                             'minor_freq',\n",
    "                             'major_allele',\n",
    "                             'major_count',\n",
    "                             'major_freq',\n",
    "                             'contig',\n",
    "                             'loc',\n",
    "                             'pop_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_allele_data(count_data):\n",
    "    allele_data = {}\n",
    "    for snp, data in count_data.to_dict().items():\n",
    "        name_data = snp.split(\"_\")\n",
    "        if len(data) == 2:\n",
    "            total = np.sum([x[1] for x in data])\n",
    "            allele_data[snp] = SNP(snp, data[0][0],\n",
    "                                              data[0][1],\n",
    "                                              data[0][1]/total,\n",
    "                                              data[1][0],\n",
    "                                              data[1][1],\n",
    "                                              data[1][1]/total,\n",
    "                                              \"_\".join(name_data[0:-1]),\n",
    "                                              int(name_data[-1]),\n",
    "                                              None)\n",
    "        else:\n",
    "            total = data[0][1]\n",
    "            allele_data[snp] = SNP(snp, None,\n",
    "                                              None,\n",
    "                                                None,\n",
    "                                              data[0][0],\n",
    "                                              data[0][1],\n",
    "                                              data[0][1]/total,\n",
    "                                              \"_\".join(name_data[0:-1]),\n",
    "                                              int(name_data[-1]),\n",
    "                                              None)\n",
    "    return allele_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allele_data = get_allele_data(gt_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['population'] = df.apply(lambda x: x.name.split(\"_\")[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pop_allele_data = {}\n",
    "\n",
    "def add_allele_freq(gt_list):\n",
    "    data = gt_list\n",
    "    if len(gt_list) == 2:\n",
    "        total = data[0][1]+data[1][1]\n",
    "        return {data[0][0]:[data[0][1], data[0][1]/total],\n",
    "               data[1][0]:[data[1][1], data[1][1]/total]}\n",
    "    else:\n",
    "        return {data[0][0]:[data[0][1], 1.0]}\n",
    "\n",
    "for group, data in df.groupby('population'):\n",
    "    data = data.drop('population', axis=1)\n",
    "    print(group, data.shape)\n",
    "    gt = data.apply(count_genotypes).apply(add_allele_freq)\n",
    "    pop_allele_data[group] = gt.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allele_data['ctg7180006068889_3947']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pop_allele_data['NC']['ctg7180005818477_209']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pimass_gt = [read_df(x, '_pimass_gt').replace(\"NA\", np.nan) for x in analysis_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pimass_gt[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pimass_gt[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gt_ref_alt = [read_df(x, 'gt_ref_alt_df') for x in analysis_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "gt_ref_alt_minor_major = [read_df(x, 'gt_ref_alt_minor_major') for x in analysis_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gt_ref_alt[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translation_df = pd.read_csv(\"translation_table.csv\", sep=\"\\t\", index_col=0)\n",
    "def get_translated_name(n):\n",
    "    n = n.strip()\n",
    "    if n in translation_df.index:\n",
    "        row = translation_df.ix[n.strip()]\n",
    "        return \"%s_%d_%d\" % (row['pop'], row.indiv, row.dup)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for p in pimass_gt:\n",
    "    p.columns = [get_translated_name(x) for x in p.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pimass_pheno = [read_df(x, '_pimass_pheno') for x in analysis_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pimass_pheno[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pp in pimass_pheno:\n",
    "    pp.columns = [x.lower().replace(\" \", \"_\") for x in pp.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phenos = [x[['mass_resid', 'pupual_duration_resid', 'total_dev_time_resid']] for x in pimass_pheno]\n",
    "#phenos = [x[['massx', 'pdx', 'tdtx']] for x in pimass_pheno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rc = Client(profile=\"sge\")\n",
    "dview = rc[:]\n",
    "lview = rc.load_balanced_view()\n",
    "len(dview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@lview.remote()\n",
    "def calc_alpha(args):\n",
    "    pheno, data = args\n",
    "    res = {}\n",
    "    import statsmodels.api as sm  \n",
    "    for snp in data.columns[3:]:\n",
    "        model = sm.OLS(data[pheno], data[snp], missing='drop')\n",
    "        fit = model.fit()\n",
    "        res[snp] = fit.params.values[0]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alphas = []\n",
    "for i, pheno_df in enumerate(phenos):\n",
    "    alphas.append({})\n",
    "    data = pheno_df.join(pimass_gt[i].T.ix[2:,])\n",
    "    data.columns = [x.replace(\" \", \"_\") for x in data.columns]\n",
    "    data = data.astype(float)\n",
    "    for pheno in data.columns[0:3]:\n",
    "        alphas[i][pheno] = calc_alpha((pheno, data))\n",
    "        print(i, pheno)\n",
    "#         for snp in data.columns[3:]:\n",
    "#             alphas[i][pheno][snp] = calc_alpha(data)\n",
    "#             model = sm.OLS(data[pheno], data[snp], missing='drop')\n",
    "#             fit = model.fit()\n",
    "#             alphas[i][pheno][snp] = fit.params.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, alpha in enumerate(alphas):\n",
    "    for pheno in alpha:\n",
    "        print(i, pheno, alpha[pheno].ready())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, alpha in enumerate(alphas):\n",
    "    for pheno in alpha:\n",
    "        alphas[i][pheno] = alpha[pheno].r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pimass_pheno[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pimass_gt[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_df = [pd.DataFrame(x) for x in alphas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_df[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, ad in enumerate(alpha_df):\n",
    "    save_df(analysis_dir[i], 'alpha_df', ad)\n",
    "#     hdfs[i]['alpha_df'] = ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pheno_names = phenos[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pheno_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sig_pheno_name(col):\n",
    "    return {'mass_resid': 'mass',\n",
    "           'pupual_duration_resid': 'pd',\n",
    "           'total_dev_time_resid': 'tdt',\n",
    "           'massx': 'mass',\n",
    "           'tdtx': 'tdt',\n",
    "           'pdx':'pd'}[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def assign_a1_a2(snp):    \n",
    "    if snp.alpha > 0:\n",
    "        return (snp.minor_allele, snp.major_allele, snp.minor_freq, \"minor\")\n",
    "    else:\n",
    "        return (snp.major_allele, snp.minor_allele, snp.major_freq, \"major\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "squat_files = []\n",
    "\n",
    "for i, hdf in enumerate(hdfs):\n",
    "    outdir = analysis_dir[i]\n",
    "    for name in pheno_names:\n",
    "        paf = pop_allele_freqs[i]\n",
    "        pheno_hdf = get_sig_pheno_name(name)\n",
    "        for sig_key, sig in list({\"relaxed\": hdf['relaxed_sig_snps_%s' % pheno_hdf],\n",
    "                            \"sig\": hdf['sig_snps_%s' % pheno_hdf]}.items()):\n",
    "            print(sig_key, name, pheno_hdf, len(sig))\n",
    "            full = hdf['mcmc_%s_hmean' % pheno_hdf]\n",
    "            full.index = full.rs\n",
    "            full.index.name=\"%s_%d\" % (name, i)\n",
    "            alpha = alphas[i][name]\n",
    "            full['alpha'] = full.apply(lambda x: alpha[x.name], axis=1)       \n",
    "            full = full.drop(\"rs\", axis=1)\n",
    "            full = full.join(gt_ref_alt[i].T[['ref', 'alt']])\n",
    "            af = hdfs[i]['allele_freqs'] #this is ref, alt.  ref will be A1, `p` is its freq\n",
    "            full['frq'] = full.apply(lambda x: af[x.name]['p'], axis=1)\n",
    "            \n",
    "            # write files\n",
    "            gwas_data_file = write_gwas_data_file(full.ix[sig.rs], outdir, name, i, sig_key)\n",
    "            gwas_freqs_file, gwas_pops = write_freqs_file(full.ix[sig.rs], paf, outdir, name, i, \"gwas\", sig_key)\n",
    "            #env_var_files = write_env_var_data_files(paf, gwas_pops, outdir, i, name, sig_key)\n",
    "            match_pop_file = write_match_pop_file(full, paf, outdir, i, \"ALL\", name, sig_key)\n",
    "            full_freqs_file, full_pops = write_freqs_file(full, paf, outdir, name, i, \"full\", sig_key)\n",
    "            squat_vars = get_squat_vars(name, gwas_data_file,\n",
    "                          gwas_freqs_file, \n",
    "                          bioclim_env_files,\n",
    "                          match_pop_file,\n",
    "                          full_freqs_file, len(full))\n",
    "            squat_file = create_squat_run_file(name, outdir, squat_vars, sig_key)\n",
    "            print(\"wrote %s\" % squat_file)\n",
    "            squat_files.append(squat_file)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "with open(os.path.join(os.path.commonprefix(squat_files), \"squat_parallel.txt\"), \"w\") as o:\n",
    "    print(o.name)\n",
    "    for rsf in squat_files:\n",
    "        o.write(\"cd %s && R --slave --vanilla < %s > /dev/null\\n\" % (os.path.dirname(rsf), rsf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_A1_pop_freq(snp, pop):\n",
    "    allele = snp['A1']\n",
    "    if allele in paf[pop][snp.name]:\n",
    "        return paf[pop][snp.name][allele][-1]\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def write_freqs_file(df, paf, outdir, pheno, imputed, key, sig_key):\n",
    "    df = df.copy()\n",
    "    outfile = os.path.join(outdir, \"freqs.file.%s.%s.%s.%s.txt\" % (sig_key, key, pheno, imputed))\n",
    "    df.index.name = \"SNP\"\n",
    "    dfs = []\n",
    "    for pop in paf:\n",
    "        df2 = df.copy()\n",
    "        df2['A1_pop_freq'] = df2.apply(get_A1_pop_freq, args=(pop,), axis=1)\n",
    "        df2['CLST'] = pop\n",
    "        df2 = df2[['CLST', 'A1', 'A2', 'A1_pop_freq', 'loc', 'contig']]\n",
    "        df2.columns = ['CLST', 'A1', 'A2', 'FRQ', 'POS', 'CHR']\n",
    "        dfs.append(df2)\n",
    "    combined = pd.concat(dfs)\n",
    "    combined.to_csv(outfile, header=True, index=True, sep=\"\\t\")\n",
    "    return outfile, combined['CLST'].unique()\n",
    "\n",
    "def write_gwas_data_file(df, outdir, pheno, imputed, sig_key):\n",
    "    df = df.copy()\n",
    "    outfile = os.path.join(outdir, \"gwas.data.file.%s.%s.%s.txt\" % (sig_key, pheno, imputed))\n",
    "    df.index.name = \"SNP\"\n",
    "    df = df[['A1', 'A2', 'alpha', 'A1_freq']]\n",
    "    df.columns = [\"A1\", \"A2\", \"EFF\", \"FRQ\"]\n",
    "    df.to_csv(outfile, header=True, index=True, sep=\"\\t\")\n",
    "    return outfile\n",
    "    \n",
    "\n",
    "def write_env_var_data_files(paf, pops, outdir, imputed, pheno, sig_key):\n",
    "    outfiles = []\n",
    "    outfile = os.path.join(outdir, \"env.var.data.file.%s.%s.%s.txt\" % (sig_key, pheno, imputed))\n",
    "    outfiles.append(outfile)\n",
    "    with open(outfile, \"w\") as o:\n",
    "        o.write(\"CLST\\tENV\\tREG\\n\")\n",
    "        pop_id = 1\n",
    "        for pop in sorted(paf):\n",
    "            if pop in pops:\n",
    "                o.write(\"%s\\t%g\\t%d\\n\" % (pop, np.random.randn(), pop_id))\n",
    "            pop_id += 1\n",
    "    return outfiles\n",
    "  \n",
    "\n",
    "def write_match_pop_file(df, paf, outdir, imputed, matchpop, pheno, sig_key):\n",
    "    outfile = os.path.join(outdir, \"match.pop.file.%s.%s.%s.%s.txt\" % (sig_key, pheno, matchpop, imputed))\n",
    "    df2 = df.copy()\n",
    "    df2.index.name=\"SNP\"\n",
    "    df2['CLST'] = matchpop\n",
    "    df2['A1_freq'] = df2.apply(get_A1_pop_freq, args=(matchpop,), axis=1)\n",
    "    df2 = df2[['CLST', 'A1', 'A2', 'A1_freq', 'loc', 'contig']]\n",
    "    df2.columns = ['CLST', 'A1', 'A2', 'FRQ', 'POS', 'CHR']\n",
    "    df2.to_csv(outfile, header=True, index=True, sep=\"\\t\")\n",
    "    return outfile\n",
    "\n",
    "def get_squat_vars(pheno,\n",
    "                   gwas_data_file, \n",
    "                  gwas_freqs_file, \n",
    "                  env_var_files,\n",
    "                  match_pop_file,\n",
    "                  full_freqs_file,\n",
    "                  num_snps):\n",
    "    d = {\"gwas.data.file\": \"'%s'\" % gwas_data_file,\n",
    "         \"freqs.file\": \"'%s'\" % gwas_freqs_file,\n",
    "         \"env.var.data.files\": \"list(%s)\" % ','.join([\"'%s'\" % x for x in env_var_files]),\n",
    "         \"match.pop.file\": \"'%s'\" % match_pop_file,\n",
    "         \"full.dataset.file\": \"'%s'\" % full_freqs_file,\n",
    "         \"path\":\"'%s'\" % pheno,\n",
    "         \"match.categories\":\"c('FRQ')\",\n",
    "         \"match.bins\":\"list(seq(0,0.5,0.02))\",\n",
    "         \"cov.SNPs.per.cycle\":20000,\n",
    "         \"cov.cycles\":100,\n",
    "         \"null.phenos.per.cycle\":10000,\n",
    "         \"null.cycles\":10,\n",
    "         \"load.cov.mat\":\"F\",\n",
    "         \"sim.null\":\"T\",\n",
    "         \"check.allele.orientation\":\"T\"}\n",
    "    return ',\\n'.join(\"%s=%s\" % (key,val) for (key,val) in list(d.items()))\n",
    "\n",
    "def create_squat_run_file(pheno, outdir, squat_vars, sig_key):\n",
    "    squat_dir = \"/home/cfriedline/eckertlab/src/PolygenicAdaptationCode/Scripts/\"\n",
    "    out_dir = os.path.join(outdir, \"squat_%s\" % sig_key)\n",
    "    res_dir = os.path.join(out_dir, pheno)\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.mkdir(out_dir)\n",
    "        os.symlink(squat_dir, os.path.join(out_dir, \"Scripts\"))\n",
    "    squat_file = os.path.join(out_dir, \"squat_%s.%s.r\" % (sig_key, pheno))\n",
    "    with open(squat_file, \"w\") as o:\n",
    "        o.write(\"system('rm -rf %s')\\n\" % res_dir)\n",
    "        o.write(\"setwd('/home/cfriedline/eckertlab/src/PolygenicAdaptationCode')\\n\")\n",
    "        o.write(\"source('%s')\\n\" % os.path.join(squat_dir, \"CreateTraitFile.R\"))\n",
    "        o.write(\"source('%s')\\n\" % os.path.join(squat_dir, \"functions.R\"))\n",
    "        o.write(\"setwd('%s')\\n\" % out_dir)\n",
    "        o.write(\"PolygenicAdaptationFunction(%s)\\n\" % squat_vars)\n",
    "    return squat_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allele_df = pd.DataFrame(allele_data, index=SNP._fields).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def current_time():\n",
    "    return str(datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monomorphic_pop_snps = set()\n",
    "for pop in pop_allele_data:\n",
    "    d = pop_allele_data[pop]\n",
    "    for snp in d:\n",
    "        if len(d[snp]) == 1:\n",
    "            monomorphic_pop_snps.add(snp)\n",
    "len(monomorphic_pop_snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full.ix[sig.rs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'ctg7180005879129_2189' in monomorphic_pop_snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create squat files for range snps\n",
    "range_squat_files = []\n",
    "from collections import defaultdict\n",
    "squat_dict = defaultdict()\n",
    "for i, d in enumerate(analysis_dir):\n",
    "    squat_dict[d] = defaultdict()\n",
    "    if i == 0:\n",
    "        continue\n",
    "    outdir = analysis_dir[i]\n",
    "    for name in pheno_names:\n",
    "        squat_dict[d][name] = defaultdict()\n",
    "        paf = pop_allele_data\n",
    "        pheno_hdf = get_sig_pheno_name(name)\n",
    "        snp_ranges = read_df(d, '%s_quant_range' % pheno_hdf)\n",
    "        full = read_df(d, 'mcmc_%s_hmean' % pheno_hdf)\n",
    "        full.index = full.rs\n",
    "        full.index.name=\"%s_%d\" % (name, i)\n",
    "        full = full.drop(monomorphic_pop_snps, errors=\"ignore\")\n",
    "        alpha = alphas[i][name]\n",
    "        full['alpha'] = full.apply(lambda x: alpha[x.name], axis=1)       \n",
    "        full = full.drop(\"rs\", axis=1)\n",
    "        full = full.join(allele_df, how=\"inner\")\n",
    "        full['allele_data'] = full.apply(assign_a1_a2, axis=1)\n",
    "        full[['A1','A2','A1_freq','effect_allele']] = full['allele_data'].apply(pd.Series)\n",
    "        print(current_time(), name, len(full))\n",
    "        try:\n",
    "            for j, val in enumerate(snp_ranges[\"0\"].values):\n",
    "                exp_dir = os.path.join(outdir, \"squat_ranges\")\n",
    "                exp_dir = os.path.join(exp_dir, \"%s_range_%d\" % (pheno_hdf, j))\n",
    "                if not os.path.exists(exp_dir):\n",
    "                    os.makedirs(exp_dir)\n",
    "                else:\n",
    "                    shutil.rmtree(exp_dir)\n",
    "                    os.makedirs(exp_dir)\n",
    "                sig = read_df(d, 'range_snps_%s_%d' % (pheno_hdf, j))\n",
    "                sig.index = sig.rs\n",
    "                sig = sig.drop(monomorphic_pop_snps, errors=\"ignore\")\n",
    "                sig_key = \"range_%d\" % j\n",
    "                print(current_time(), sig_key)\n",
    "                gwas_data_file = write_gwas_data_file(full.ix[sig.rs], exp_dir, name, i, sig_key)\n",
    "                gwas_freqs_file, gwas_pops = write_freqs_file(full.ix[sig.rs], paf, exp_dir, name, i, \"gwas\", sig_key)\n",
    "                match_pop_file = write_match_pop_file(full, paf, exp_dir, i, \"QC32\", name, sig_key)\n",
    "                full_freqs_file, full_pops = write_freqs_file(full, paf, exp_dir, name, i, \"full\", sig_key)\n",
    "                squat_vars = get_squat_vars(name, gwas_data_file,\n",
    "                              gwas_freqs_file, \n",
    "                              bioclim_env_files,\n",
    "                              match_pop_file,\n",
    "                              full_freqs_file, len(full))\n",
    "                squat_file = create_squat_run_file(name, exp_dir, squat_vars, sig_key)\n",
    "                print(current_time(), \"wrote %s\" % squat_file)\n",
    "                range_squat_files.append(squat_file)\n",
    "                \n",
    "                squat_dict[d][name][val] = {'gwas_data_file': gwas_data_file,\n",
    "                                           'gwas_freqs_file': gwas_freqs_file,\n",
    "                                           'gwas_pops': gwas_pops,\n",
    "                                           'match_pop_file': match_pop_file,\n",
    "                                           'full_freqs_file': full_freqs_file,\n",
    "                                           'full_pops': full_pops,\n",
    "                                           'squat_file': squat_file}\n",
    "                #break\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        #break\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ni = open(\"/home/cfriedline/eckertlab/gypsy_indiv/raw_demult/analysis/samtools1.3_masurca3/ni/squat_parallel.txt\", \"w\")\n",
    "bg = open(\"/home/cfriedline/eckertlab/gypsy_indiv/raw_demult/analysis/samtools1.3_masurca3/beagle40/squat_parallel.txt\", \"w\")\n",
    "ni_files = []\n",
    "bg_files = []\n",
    "for rsf in range_squat_files:\n",
    "    o = ni\n",
    "    if 'beagle40' in rsf:\n",
    "        o = bg\n",
    "        bg_files.append(rsf)\n",
    "    else:\n",
    "        ni_files.append(rsf)\n",
    "    o.write(\"cd %s && R --slave --vanilla < %s > %s.log\\n\" % (os.path.dirname(rsf), rsf, rsf))\n",
    "[x.close() for x in [ni, bg]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "~/bin/parallel --no-notice --bar -a squat_parallel.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "range_outputs = !find {os.path.commonprefix(range_squat_files)} | grep 'Output$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "range_outputs = !find {os.path.commonprefix(bg_files)} | grep 'Output$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(range_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "range_output_data = []\n",
    "used_outputs = []\n",
    "for output in range_outputs:\n",
    "    p = PurePath(output)\n",
    "    d = p.parents[4]\n",
    "    pheno_range = os.path.basename(os.path.dirname(os.path.dirname(os.path.dirname(output))))\n",
    "    pheno_data = pheno_range.split(\"_\")\n",
    "    pheno = pheno_data[0]\n",
    "    range_id = int(pheno_data[-1])\n",
    "    ranges = read_df(str(d), '%s_quant_range' % pheno)\n",
    "    r(\"rm(list=ls())\")\n",
    "    for obj in os.listdir(output):\n",
    "        obj = os.path.join(output, obj)\n",
    "        r(\"load('%s')\" % obj)\n",
    "    try:\n",
    "        range_output_data.append({'pheno': pheno,\n",
    "                                'pheno_range': pheno_range,\n",
    "                                      'cutoff':ranges.ix[range_id,\"0\"],\n",
    "                                          'Qx': r('the.stats$Qx')[0],\n",
    "                                          'Pr(Qx)': r('p.vals$Qx')[0],\n",
    "                                          'Fst': r('the.stats$Fst.comp')[0],\n",
    "                                          'Pr(Fst)': r('p.vals$Fst.comp')[0],\n",
    "                                          'LD': r('the.stats$LD.comp')[0],\n",
    "                                          'Pr(LD)': r('p.vals$LD.comp')[0]})\n",
    "        used_outputs.append(output)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_gwas = {}\n",
    "for uo in used_outputs:\n",
    "    res = !wc -l {os.path.dirname(os.path.dirname(os.path.dirname(uo)))}/gwas*.txt\n",
    "    res = res[0].split()\n",
    "    n = res[0]\n",
    "    p = os.path.basename(os.path.dirname(res[1]))\n",
    "    num_gwas[p] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#range_output_data = []\n",
    "for output in range_outputs:\n",
    "    p = PurePath(output)\n",
    "    d = p.parents[4]\n",
    "    pheno_range = os.path.basename(os.path.dirname(os.path.dirname(os.path.dirname(output))))\n",
    "    pheno_data = pheno_range.split(\"_\")\n",
    "    pheno = pheno_data[0]\n",
    "    range_id = int(pheno_data[-1])\n",
    "    if range_id ==  9:\n",
    "        ranges = read_df(str(d), '%s_quant_range' % pheno)\n",
    "        r(\"rm(list=ls())\")\n",
    "        for obj in os.listdir(output):\n",
    "            obj = os.path.join(output, obj)\n",
    "            print(obj)\n",
    "            r(\"load('%s')\" % obj)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "hist(null.stats$Qx, xlim=c(0, the.stats$Qx+50))\n",
    "abline(v=the.stats$Qx, col=\"red\")\n",
    "text(150, 1000, \"Qx\", col=\"red\")\n",
    "hist(null.stats$LD.component, xlim=c(-5, the.stats$LD.component+50))\n",
    "abline(v=the.stats$LD.component, col=\"red\")\n",
    "text(150, 1000, \"LD\", col=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "range_output_df = pd.DataFrame(range_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "range_output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "range_output_df['num'] = range_output_df.apply(lambda x: num_gwas[x.pheno_range], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "range_output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_df(analysis_dir[1], \"range_output_df\", range_output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "sns.set_context(\"talk\")\n",
    "for group, data in range_output_df.groupby(\"pheno\"):\n",
    "    data = data.sort_values(\"cutoff\", ascending=False)\n",
    "    print(len(data))\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(30,5)\n",
    "    ax1 = fig.add_subplot(131)\n",
    "    ax1.plot(data.cutoff, data.Qx, color=\"blue\")\n",
    "    plt.ylabel(\"Qx\")\n",
    "    plt.xlabel(\"PIP cutoff\")\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(data.cutoff, data['Pr(Qx)'], color=\"red\")\n",
    "    plt.ylabel(\"Pr(Qx)\")\n",
    "    ax2.grid(None)\n",
    "    plt.title(group)\n",
    "    red_patch = mpatches.Patch(color='red', label='Pr(Qx)')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Qx')\n",
    "    for row in data.index:\n",
    "        ax1.annotate(data.ix[row,'num'], \n",
    "                     (data.ix[row, 'cutoff'], data.ix[row, 'Qx']),\n",
    "                    xytext = (-1, 1),\n",
    "                    textcoords = 'offset points',\n",
    "                    ha = 'right', va = 'bottom',\n",
    "                    bbox = dict(boxstyle = 'round,pad=0.5', fc = 'yellow', alpha = 0.5))\n",
    "    \n",
    "    plt.legend(handles=[blue_patch, red_patch], loc=\"upper center\")\n",
    "    \n",
    "    ax3 = plt.subplot(132)\n",
    "    ax3.plot(data.cutoff, data.LD, color=\"blue\")\n",
    "    plt.ylabel(\"LD\")\n",
    "    ax4 = ax3.twinx()\n",
    "    ax4.plot(data.cutoff, data['Pr(LD)'], color=\"red\")\n",
    "    plt.ylabel(\"Pr(LD)\")\n",
    "    ax4.grid(None)\n",
    "    red_patch = mpatches.Patch(color='red', label='Pr(LD)')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='LD')\n",
    "    plt.legend(handles=[blue_patch, red_patch], loc=\"upper center\")\n",
    "    plt.title(group)\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    \n",
    "    ax5 = plt.subplot(133)\n",
    "    ax5.plot(data.cutoff, data.Fst, color=\"blue\")\n",
    "    plt.ylabel(\"Fst\")\n",
    "    ax6 = ax5.twinx()\n",
    "    ax6.plot(data.cutoff, data['Pr(Fst)'], color=\"red\")\n",
    "    plt.ylabel(\"Pr(Fst)\")\n",
    "    ax6.grid(None)\n",
    "    red_patch = mpatches.Patch(color='red', label='Pr(Fst)')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Fst')\n",
    "    plt.legend(handles=[blue_patch, red_patch], loc=\"upper center\")\n",
    "    plt.title(group)\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "squat_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "setwd(\"~/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for squat_file in squat_files:\n",
    "    print(squat_file)\n",
    "    r(\"source('%s')\" % squat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(os.path.dirname(os.path.dirname(analysis_dir_imp)), \"squat_results.txt\"), \"w\") as o:  \n",
    "    o.write(\"%s\\n\" % \"\\t\".join([\"significance\", \"phenotype\", \"imputation\", \"Qx\", \"Pr(Qx)\", \n",
    "                               \"Fst\", \"Pr(Fst)\", \"LD\", \"Pr(LD)\"]))\n",
    "    for squat_file in squat_files:\n",
    "        squat_dir = os.path.dirname(squat_file)\n",
    "        datatype = os.path.basename(os.path.dirname(squat_dir))\n",
    "        pheno = os.path.basename(squat_file).split(\".\")[1]\n",
    "        pheno_dir = os.path.join(squat_dir, pheno)\n",
    "        output_dir = os.path.join(pheno_dir, \"Output\")\n",
    "        assert os.path.exists(output_dir)\n",
    "\n",
    "        for obj in os.listdir(output_dir):\n",
    "            obj = os.path.join(output_dir, obj)\n",
    "            r(\"load('%s')\" % obj)\n",
    "            \n",
    "        res = \"\\t\".join([str(x) for x in [os.path.basename(squat_dir),\n",
    "                                          pheno,\n",
    "                                          datatype,\n",
    "                                          r('the.stats$Qx')[0],\n",
    "                                          r('p.vals$Qx')[0],\n",
    "                                          r('the.stats$Fst.comp')[0],\n",
    "                                          r('p.vals$Fst.comp')[0],\n",
    "                                          r('the.stats$LD.comp')[0],\n",
    "                                          r('p.vals$LD.comp')[0]]])\n",
    "        print(res)\n",
    "        o.write(\"%s\\n\" % res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
