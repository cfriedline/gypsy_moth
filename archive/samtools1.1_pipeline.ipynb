{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.parallel import Client\n",
    "import os, time\n",
    "import include_utils as u\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import numbers\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import vcf\n",
    "from sklearn import preprocessing\n",
    "from subprocess import Popen, PIPE\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R_HOME = '/home/cfriedline/R3/lib64/R'\n",
    "os.environ['LD_LIBRARY_PATH'] = \"%s/lib:%s\" % (os.environ['R_HOME'], os.environ['LD_LIBRARY_PATH'])\n",
    "%reload_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import rpy2.robjects as robjects\n",
    "import pandas.rpy.common as com\n",
    "r = robjects.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rc = u.get_client(\"sge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dview, lview = u.get_views(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dview, lview = u.get_idle_engines(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(dview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with dview.sync_imports():\n",
    "    import os\n",
    "    import sys\n",
    "    import socket\n",
    "    import stopwatch\n",
    "    from subprocess import Popen, PIPE\n",
    "    import tempfile\n",
    "    import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hlview = u.get_single_host_lview(rc, \"all\")\n",
    "len(hlview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shell(cmd):\n",
    "    p = Popen(cmd, shell=True, stdout=PIPE, stderr=PIPE)\n",
    "    stdout, stderr = p.communicate()\n",
    "    return stdout.split(\"\\n\"), stderr.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bam_dir = \"/data7/eckertlab/gypsy_indiv/analysis2\"\n",
    "analysis_dir = os.path.join(bam_dir, \"samtools1.2\")\n",
    "if not os.path.exists(analysis_dir):\n",
    "    os.makedirs(analysis_dir)\n",
    "assert os.path.exists(analysis_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bam_files,err = shell(\"find /data7/eckertlab/gypsy_indiv | grep new | grep 'rg.bam$'\")\n",
    "bam_files = [os.path.abspath(x) for x in bam_files if '.bam' in x]\n",
    "len(bam_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samtools = \"/home/cfriedline/data7/src/samtools-1.2/samtools\"\n",
    "bcftools = \"/home/cfriedline/data7/src/bcftools-1.2/bcftools\"\n",
    "picard = \"/home/cfriedline/data7/src/broadinstitute-picard-03a1d72/dist/picard.jar\"\n",
    "java = \"/home/cfriedline/jdk1.7.0_25/bin/java\"\n",
    "perl = \"/home/cfriedline/data7/opt/ActivePerl-5.16/bin/perl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mark_duplicates(args):\n",
    "    java, picard, bam_file, analysis_dir = args\n",
    "    out_bam = os.path.join(\"%s_dedup.bam\" % os.path.basename(bam_file))\n",
    "    out_bam = os.path.join(analysis_dir, out_bam)\n",
    "    t = tempfile.NamedTemporaryFile(delete=False)\n",
    "    cmd = \"%s -jar %s MarkDuplicates \\\n",
    "    INPUT=%s OUTPUT=%s METRICS_FILE=%s.metrics\" %     (java,\n",
    "                              picard,\n",
    "                              bam_file,\n",
    "                              t.name,\n",
    "                              out_bam)\n",
    "    print cmd\n",
    "    !$cmd\n",
    "    shutil.move(t.name, out_bam)\n",
    "    return cmd, out_bam\n",
    "dview['mark_duplicates'] = mark_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmdup_jobs = []\n",
    "for b in bam_files:\n",
    "    rmdup_jobs.append(hlview.apply_async(mark_duplicates, (java, picard, b, analysis_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u.get_async_progress(rmdup_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assembly = \"/home/cfriedline/data7/assemblies/gypsy/masurca_new/CA/10-gapclose/genome.ctg.fasta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bam_rmdup_files = sorted([x.r[1] for x in rmdup_jobs])\n",
    "bam_rmdup_files[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for b in bam_rmdup_files:\n",
    "    rg = !$samtools view -H {b} | grep '^@RG'\n",
    "    print \"\\t\".join(rg)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_ploidy_file(args):\n",
    "    bam_files, analysis_dir = args\n",
    "    ploidy_file = os.path.join(analysis_dir, \"%s.ploidy\" % \"all\")\n",
    "    with open(ploidy_file, \"w\") as o:\n",
    "        for b in bam_files:\n",
    "            name = \"%s\" % os.path.basename(b).split(\".fastq\")[0]\n",
    "            ploidy = 2\n",
    "            o.write(\"%s\\t%d\\n\" % (name, ploidy))\n",
    "    return ploidy_file\n",
    "dview['create_ploidy_file'] = create_ploidy_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def call_snps(args):\n",
    "    print socket.gethostname()\n",
    "    timer = stopwatch.Timer()\n",
    "    samtools, reference, bam_sorted, bcftools, raw_vcf, out_dir = args \n",
    "    if not out_dir:\n",
    "        out_dir = os.environ['TMPDIR']\n",
    "    raw_vcf = os.path.join(out_dir, raw_vcf)\n",
    "    ploidy_file = create_ploidy_file((bam_sorted, out_dir))\n",
    "    pileup = \"%s mpileup -ugf %s %s | %s call -S %s -vmO z -o %s\" % (samtools, \n",
    "                                                                     reference, \n",
    "                                                                     ' '.join(bam_sorted), \n",
    "                                                                     bcftools, \n",
    "                                                                     ploidy_file, \n",
    "                                                                     raw_vcf) \n",
    "    \n",
    "    print pileup\n",
    "    !$pileup\n",
    "    timer.stop()\n",
    "    return pileup, timer.elapsed\n",
    "dview['call_snps'] = call_snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args = [samtools, \n",
    "        assembly, \n",
    "        bam_rmdup_files, \n",
    "        bcftools, \n",
    "        \"samtools_1.2.vcf.gz\", \n",
    "        analysis_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samtools_job = lview.apply_async(call_snps, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print samtools_job.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samtools_job.ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%connect_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##some snp analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vcf_file = os.path.join(analysis_dir, \"samtools_1.2.vcf.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vcftools = \"/home/cfriedline/data7/src/vcftools_0.1.12b/bin/vcftools\"\n",
    "bcftools = \"/home/cfriedline/data7/src//bcftools-1.2/bcftools\"\n",
    "tabix = \"/home/cfriedline/data7/src/samtools-1.2/htslib-1.2.1/tabix\"\n",
    "bgzip = \"/home/cfriedline/data7/src/samtools-1.2/htslib-1.2.1//bgzip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!$vcftools --remove-indels \\\n",
    "--min-alleles 2 \\\n",
    "--max-alleles 2 \\\n",
    "--mac 1 \\\n",
    "--remove-filtered-all \\\n",
    "--012 \\\n",
    "--gzvcf \\\n",
    "$vcf_file \\\n",
    "--out $vcf_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## snp filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hdfstorehelper import HDFStoreHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf = HDFStoreHelper(\"gypsy_samtools12.hd5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12_file = os.path.join(analysis_dir, \"%s.012\" % vcf_file)\n",
    "z12_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12_data = []\n",
    "for i, line in enumerate(open(z12_file)):\n",
    "    line = line.strip()\n",
    "    line = [int(x) for x in line.split(\"\\t\")]\n",
    "    z12_data.append(np.array(line))\n",
    "    if i % 10 == 0:\n",
    "        print i\n",
    "z12_data = np.array(z12_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12_df = pd.DataFrame(z12_data)\n",
    "z12_df = z12_df.drop(0, axis=1)\n",
    "z12_df.columns = pd.Series(z12_df.columns)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12_df[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put(\"z12_df\", z12_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(z12_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_percent_missing(col):\n",
    "    return len(col[col==-1])*1.0/len(col)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percent_missing = z12_df.apply(get_percent_missing, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percent_missing[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put('percent_missing', percent_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percent_missing[percent_missing >= 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12_df_50_perc = z12_df.ix[:,percent_missing <= 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12_df_50_perc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put('z12_df_50_perc', z12_df_50_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_monomorphic(col):\n",
    "    u = col[col != -1].value_counts()\n",
    "    if len(u) == 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "monomorphic_loci = z12_df_50_perc.apply(is_monomorphic, axis=0)\n",
    "monomorphic_loci = monomorphic_loci[monomorphic_loci==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(monomorphic_loci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12_df_50_perc_polymorphic = z12_df_50_perc.drop(monomorphic_loci.index, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put('z12_df_50_perc_polymorphic', z12_df_50_perc_polymorphic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12_df_50_perc_polymorphic[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "translation_df = pd.read_csv(\"translation_table.csv\", sep=\"\\t\", index_col=0)\n",
    "indv = os.path.join(analysis_dir, \"%s.indv\" % os.path.basename(z12_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "translation_df.ix[\"NC_131_ATAATCCA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_translated_name(n):\n",
    "    row = translation_df.ix[n.strip()]\n",
    "    return \"%s_%d_%d\" % (row['pop'], row.indiv, row.dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = [get_translated_name(x) for x in open(indv).readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12_df_50_perc_polymorphic.index = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12_df_50_perc_polymorphic['population'] = z12_df_50_perc_polymorphic.apply(lambda row: row.name.split(\"_\")[0], \n",
    "                                                                     axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12_df_50_perc_polymorphic['duplicate'] = z12_df_50_perc_polymorphic.apply(lambda row: row.name[-1],\n",
    "                                                                           axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12_df_50_perc_polymorphic[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put('z12_df_50_perc_polymorphic', z12_df_50_perc_polymorphic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_correction(n):\n",
    "    #for finite sample size\n",
    "    return (2*n)/(2*n-1)\n",
    "\n",
    "def get_allele_freqs(locus):\n",
    "    c = locus[locus != -1].value_counts()\n",
    "    total_alleles = 2.0*sum(c)\n",
    "    num_individuals = sum(c)\n",
    "    P = 0\n",
    "    Q = 0\n",
    "    PQ = 0\n",
    "    if 0 in c:\n",
    "        P = 2*c[0]\n",
    "    if 2 in c:\n",
    "        Q = 2*c[2]\n",
    "    if 1 in c:\n",
    "        PQ = c[1]\n",
    "    P += PQ\n",
    "    Q += PQ\n",
    "    p = P/total_alleles\n",
    "    q = Q/total_alleles\n",
    "    assert p + q == 1.0\n",
    "    He = 2 * p * q * get_correction(num_individuals)\n",
    "    Ho = PQ*1.0/num_individuals\n",
    "    Fis = 1 - (Ho/He)\n",
    "    #print p, q, He, Ho, Fis\n",
    "    return pd.Series({\"p\":p, \n",
    "                      \"q\":q,\n",
    "                      \"P\":P,\n",
    "                      \"Q\":Q,\n",
    "                      \"He\":He,\n",
    "                      \"Ho\":Ho, \n",
    "                      \"Fis\":Fis})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allele_freqs = z12_df_50_perc_polymorphic.ix[:,:-2].apply(get_allele_freqs)\n",
    "mafs = allele_freqs.apply(lambda x: min(x[\"p\"], x[\"q\"]))\n",
    "mafs[mafs<0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put('allele_freqs', allele_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12_df_50_perc_polymorphic_maf = z12_df_50_perc_polymorphic.drop(mafs[mafs<0.01].index, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12_df_50_perc_polymorphic_maf[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put('z12_df_50_perc_polymorphic_maf', z12_df_50_perc_polymorphic_maf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global_fis = allele_freqs[z12_df_50_perc_polymorphic_maf.columns[:-2]].apply(lambda x: x[\"Fis\"])\n",
    "fis_outliers = global_fis[(global_fis < -0.5) | (global_fis > 0.5)]\n",
    "z12_df_50_perc_polymorphic_maf_fis = z12_df_50_perc_polymorphic_maf.drop(fis_outliers.index, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12_df_50_perc_polymorphic_maf_fis[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put('z12_df_50_perc_polymorphic_maf_fis', z12_df_50_perc_polymorphic_maf_fis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "duplicates = z12_df_50_perc_polymorphic_maf_fis[z12_df_50_perc_polymorphic_maf_fis.duplicate==\"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12_df_50_perc_polymorphic_maf_fis_dedup = z12_df_50_perc_polymorphic_maf_fis.drop(duplicates.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12_df_50_perc_polymorphic_maf_fis_dedup[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put('z12_df_50_perc_polymorphic_maf_fis_dedup', z12_df_50_perc_polymorphic_maf_fis_dedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "working_df = z12_df_50_perc_polymorphic_maf_fis_dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "working_df.to_csv(os.path.join(analysis_dir,\n",
    "                               \"z12_df_50_perc_polymorphic_maf_fis_dedup.txt\"),\n",
    "                                          header=True,\n",
    "                                          index=True,\n",
    "                                          sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Start here for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "working_df = pd.read_csv(os.path.join(analysis_dir, \n",
    "                                      \"z12_df_50_perc_polymorphic_maf_fis_dedup.txt\"),\n",
    "                                      sep=\"\\t\",\n",
    "                                      index_col=0)\n",
    "cols = [int(x) for x in working_df.columns.tolist()[:-2]]\n",
    "cols.extend(working_df.columns.tolist()[-2:])\n",
    "working_df.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "working_df = hdf.get('z12_df_50_perc_polymorphic_maf_fis_dedup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hierf_trans = {0:11, 1:12, 2:22, -1:'NA'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_hierf_trans(series):\n",
    "    return [hierf_trans[x] if x in hierf_trans else x for x in series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hierf_trans_df = working_df.apply(apply_hierf_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put('hierf_trans_df', hierf_trans_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "url = 'https://docs.google.com/uc?export=download&id=0B4xHxBFoPCoWT0NneHJadUI0OHM'\n",
    "response = urllib2.urlopen(url)\n",
    "pheno = pd.read_excel(response, \"Males-forGenomics-final\")\n",
    "pheno=pheno[['Population', 'Number', 'Mass', 'Pupual Duration', 'Total Dev Time']]\n",
    "for x in pheno.index:\n",
    "    pheno.ix[x, 'sample_pheno'] = \"%s_%d\" % (pheno.ix[x, 'Population'], pheno.ix[x, 'Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pop_id = {}\n",
    "i = 0\n",
    "for p in sorted(working_df['population'].unique()):\n",
    "    pop_id[p] = i\n",
    "    i+=1\n",
    "pop_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def assign_popid(series):\n",
    "    series['popid'] = pop_id[series['population']]\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hierf_trans_df = hierf_trans_df.apply(assign_popid, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hierf_trans_df.columns = [\"L%d\" % x if isinstance(x, numbers.Number) else x for x in hierf_trans_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hierf_trans_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hierf_trans_df.to_csv(\"hierf_trans_df.txt\", header=True, index=True, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hierf_trans_df = pd.read_csv(\"hierf_trans_df.txt\", header=0, index_col=0, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put('hierf_trans_df', hierf_trans_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hierf_trans_df = hdf.get('hierf_trans_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Make sure we're only working with SNPs (not DNPs, MNPs, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hierf_trans_df.columns[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snp_pos = pd.read_csv(os.path.join(analysis_dir, \"samtools_1.2.vcf.gz.012.pos\"), \n",
    "                              sep=\"\\t\",\n",
    "                              header=None,\n",
    "                              names=['contig', 'pos'])\n",
    "snps = pd.DataFrame([int(x[1:]) for x in hierf_trans_df.columns[:-3]])\n",
    "snps.columns = ['snp_id']\n",
    "snps.index=snps.snp_id\n",
    "snp_id_pos = pd.merge(snps, snp_pos, how=\"inner\", left_index=True, right_index=True)\n",
    "snp_id_pos.index = [\"L%d\" % x for x in snp_id_pos.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snp_id_pos[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put('snp_pos', snp_pos)\n",
    "hdf.put('snp_id_pos', snp_id_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove malformed header in vcf\n",
    "# ##INFO=<ID=VDB,Number=1,Type=Float,Description=\"Variant Distance Bias for filtering splice-site artefacts in RNA-seq data (bigger is better)\",Version=\"3\">\n",
    "# gunzip samtools_1.2.vcf.gz\n",
    "# sed '/ID=VDB/d' samtools_1.2.vcf > samtools_1.2.vcf_novdb.vcf\n",
    "# bgzip samtools_1.2.vcf_novdb.vcf\n",
    "# tabix samtools_1.2.vcf_novdb.vcf.gz\n",
    "reader = vcf.VCFReader(filename=os.path.join(analysis_dir, \"samtools_1.2.vcf_novdb.vcf.gz\"))\n",
    "def get_ref_alt_alleles(row):\n",
    "    alleles = reader.fetch(row.contig, row.pos)\n",
    "    row['ref'] = alleles.REF\n",
    "    row['alt'] = alleles.ALT\n",
    "    return row\n",
    "snp_id_pos_ref_alt = snp_id_pos.apply(get_ref_alt_alleles, axis=1)\n",
    "snp_id_pos_ref_alt['ref_len'] = snp_id_pos_ref_alt.apply(lambda x: len(x.ref), axis=1)\n",
    "snp_id_pos_ref_alt_snps_only = snp_id_pos_ref_alt[snp_id_pos_ref_alt.ref_len==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put('snp_id_pos_ref_alt', snp_id_pos_ref_alt)\n",
    "hdf.put('snp_id_pos_ref_alt_snps_only', snp_id_pos_ref_alt_snps_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snp_id_pos_ref_alt.to_csv(\"snp_id_pos_ref_alt.csv\", \n",
    "                          header=True, \n",
    "                          index=True, \n",
    "                          sep=\"\\t\")\n",
    "snp_id_pos_ref_alt_snps_only.to_csv(\"snp_id_pos_ref_alt_snps_only.csv\",\n",
    "                                    header=True, \n",
    "                                    index=True,\n",
    "                                    sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snp_id_pos_ref_alt_snps_only = pd.read_csv(\"snp_id_pos_ref_alt_snps_only.csv\", \n",
    "                                           header=0,\n",
    "                                           index_col=0,\n",
    "                                           sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snp_id_pos_ref_alt_snps_only = hdf.get('snp_id_pos_ref_alt_snps_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snp_id_pos_ref_alt_snps_only[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = ['popid']\n",
    "# cols.extend(hierf_trans_df.columns[:-3])\n",
    "cols.extend(snp_id_pos_ref_alt_snps_only.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hierf_trans_df2 = hierf_trans_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hierf_trans_df2[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put('hierf_trans_df2', hierf_trans_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hierf_trans_df2.popid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hierf_trans_df2.to_csv(\"hierfstat_samtools.txt\", header=True, index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Put into R (because it can be slow)\n",
    "\n",
    "    library(hierfstat)\n",
    "    data = read.table(\"hierfstat_samtools.txt\", header=T, sep=\"\\t\")\n",
    "    levels = data.frame(data$popid)\n",
    "    loci = data[,2:ncol(data)]\n",
    "    res = varcomp.glob(levels=levels, loci=loci, diploid=T)\n",
    "    saveRDS(res, \"hierfstat_samtools_new.rds\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "res = readRDS(\"hierfstat_samtools_new.rds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = com.convert_robj(robjects.r('res'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc_df = res['loc']\n",
    "F_df = res['F']\n",
    "overall_df = res['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_fst(series):\n",
    "    Hs = series[0]\n",
    "    Ht = sum(series)\n",
    "    return Hs/Ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loci_fst = loc_df.apply(compute_fst, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pop_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(loci_fst, bins=20)\n",
    "plt.title(\"n=%d mean=%.2f +/- %.2f [%.2f, %.2f]\" % (len(loci_fst), \n",
    "                                                    np.mean(loci_fst), \n",
    "                                                    np.std(loci_fst),\n",
    "                                                    np.min(loci_fst), \n",
    "                                                    np.max(loci_fst)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "working_df = hdf.get('z12_df_50_perc_polymorphic_maf_fis_dedup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "working_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "working_df.columns = [\"L%d\" % x if isinstance(x, numbers.Number) else x for x in working_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allele_freqs = working_df.ix[:,:-2].apply(get_allele_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allele_freqs.to_csv(\"allele_freqs.txt\", header=True,index=True, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allele_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def swap_alleles(locus):\n",
    "    if locus.name.startswith(\"L\"):\n",
    "        #locus_id = int(locus.name[1:]) #drop the L and convert\n",
    "        locus_id = locus.name\n",
    "        freqs = allele_freqs[locus_id]\n",
    "        maf = min(freqs[\"p\"], freqs[\"q\"])\n",
    "        if maf == freqs[\"p\"]:\n",
    "            return locus.replace({0:2,2:0})\n",
    "        return locus\n",
    "    else:\n",
    "        return locus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "working_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_maf = working_df.apply(swap_alleles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_maf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def center_and_standardize_value(val, u, var):\n",
    "    if val == -1:\n",
    "        return 0.0\n",
    "    return (val-u)/np.sqrt(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def center_and_standardize(locus):\n",
    "    if locus.name.startswith(\"L\"):\n",
    "        #locus_id = int(locus.name[1:])\n",
    "        locus_id = locus.name\n",
    "        freqs = allele_freqs[locus_id]\n",
    "        maf = min(freqs[\"p\"], freqs[\"q\"])\n",
    "        var = np.sqrt(maf*(1-maf))\n",
    "        u = np.mean(locus)\n",
    "        return locus.apply(center_and_standardize_value, args=(u, var))\n",
    "    return locus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in pca_maf.ix[:,0:30]:\n",
    "    print pca_maf[col].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_std = pca_maf.apply(center_and_standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_std.ix[0:5,-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_std_data = pca_std.ix[:,:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_std_data_snps = pca_std_data.ix[:,snp_id_pos_ref_alt_snps_only.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prcomp = r('prcomp')\n",
    "summary = r('summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_std_data_snps[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_std_data_snps.to_csv(\"pca_std_data_snps.txt\", header=True, index=True, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prcomp_res = prcomp(pca_std_data_snps, scale=False, center=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print summary(prcomp_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = com.convert_robj(prcomp_res.rx2(\"x\"))\n",
    "x.index = pca_std_data.index\n",
    "joined = x.join(pca_maf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf.put('joined', joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm = mcolors.Normalize(min(pop_id.values()), max(pop_id.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "legend = {}\n",
    "for row in joined.iterrows():\n",
    "    pop = row[1]['population']\n",
    "    n = norm(pop_id[pop])\n",
    "    color = cm.rainbow(n)\n",
    "    legend[pop] = color\n",
    "    plt.scatter(row[1].PC1, \n",
    "                row[1].PC2, \n",
    "                s=50, \n",
    "                c=color)\n",
    "fig = plt.gcf()\n",
    "ax = plt.gca()\n",
    "cmap = plt.get_cmap()\n",
    "fig.set_size_inches(10,8)\n",
    "plt.title(\"PCA of n=%d samples on %d loci\" % (len(joined), len(pca_std_data.columns)))\n",
    "plt.xlabel(\"PC1 (3.259%)\")\n",
    "plt.ylabel(\"PC2 (1.983%)\")\n",
    "\n",
    "handles = []\n",
    "for pop in sorted(legend):\n",
    "    handles.append(mpatches.Patch(color=legend[pop], label=pop))\n",
    "plt.legend(handles=sorted(handles))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "source(\"tw_calc.R\")\n",
    "test=read.table(\"twtable\", header=F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TWcalc = r('TWcalc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "robjects.globalenv['pca_std_data_snps'] = com.convert_to_r_matrix(pca_std_data_snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "saveRDS(pca_std_data_snps, file=\"pca_std_data_snps.rds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw = TWcalc(com.convert_to_r_matrix(pca_std_data_snps),20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw_p = com.convert_robj(tw.rx2(2))\n",
    "tw_e = com.convert_robj(tw.rx2(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw_num = 0\n",
    "for i, p in enumerate(tw_p):\n",
    "    print p\n",
    "    if p > 0.05:\n",
    "        tw_num = i\n",
    "        break\n",
    "print \"Tracy-Widom test yields %d axes of pop structure\" % tw_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracy-Widom test yields 12 axes of pop structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_cov = x.ix[:,0:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_cov[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pheno['sample_id'] = pheno.apply(lambda x: \"%s_0\" % x.sample_pheno, axis=1)\n",
    "pheno.index = pheno['sample_id']\n",
    "pheno.drop('sample_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_std_pheno = pheno.join(pca_cov, how=\"inner\").join(pca_maf[snp_id_pos_ref_alt_snps_only.index], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(pca_std_data.index) - set(pca_std_pheno.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_std_pheno[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "translation_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snp_id_pos_ref_alt_snps_only[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reader = vcf.VCFReader(filename=os.path.join(analysis_dir, \"samtools_1.2.vcf_novdb.vcf.gz\"))\n",
    "\n",
    "def get_correct_name(name):\n",
    "    row = translation_df.ix[name,:]\n",
    "    return \"%s_%d_%d\" % (row['pop'], row.indiv, row.dup)\n",
    "\n",
    "gt_base_data = {}\n",
    "for i, row in enumerate(snp_id_pos_ref_alt_snps_only.iterrows()):\n",
    "    snp_id = row[0]\n",
    "    snp = reader.fetch(row[1].contig, row[1].pos)\n",
    "    for sample in snp.samples:\n",
    "        if not snp_id in gt_base_data:\n",
    "            gt_base_data[snp_id] = {}\n",
    "        sample_name = get_correct_name(sample.sample)\n",
    "        gt_base_data[snp_id][sample_name] = sample.gt_bases\n",
    "    if i % 10 == 0:\n",
    "        print i\n",
    "gt_base_df = pd.DataFrame(gt_base_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in gt_base_df.ix[:,0:30]:\n",
    "    print col, gt_base_df[col].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put('gt_base_df', gt_base_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gt_base_df=hdf.get('gt_base_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gt_base_df.to_csv(\"gt_base_df.csv\", header=True, index=True, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gt_base_df = pd.read_csv(\"gt_base_df.csv\", index_col=0, sep=\"\\t\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gt_base_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gt_base_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(SNPassoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pheno.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pheno_cols = ['Mass','Pupual Duration','Total Dev Time']\n",
    "pheno_cols.extend(gt_base_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pheno_gt_base = pheno.merge(gt_base_df, left_index=True, right_index=True, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pheno_gt_base = pheno_gt_base[pheno_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put('pheno_gt_base', pheno_gt_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pheno_gt_base_pca = pheno_gt_base.merge(pca_cov, left_index=True, right_index=True, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pheno_gt_base_pca.columns = pheno_gt_base_pca.apply(lambda x: x.name.replace(\" \", \"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pheno_gt_base_pca.ix[:,0:3] = preprocessing.scale(pheno_gt_base_pca.ix[:,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pheno_gt_base_pca[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pheno_gt_base_pca.ix[:,0:3].apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hdf.put('pheno_gt_base_pca', pheno_gt_base_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pheno_gt_base_pca=hdf.get('pheno_gt_base_pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pheno_gt_base_pca.to_csv(\"pheno_gt_base_pca.txt\",\n",
    "                         header=True,\n",
    "                         index=True,\n",
    "                         sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do this in R b/c slow\n",
    "    \n",
    "    library(SNPassoc)\n",
    "    \n",
    "    pheno_gt_base_pca = read.table(\"pheno_gt_base_pca.txt\", sep=\"\\t\", row.names=1, header=T)\n",
    "    pheno_gt_base_pca[pheno_gt_base_pca==\"None\"] = NA\n",
    "\n",
    "    #subtract b/c those are the PCA axes\n",
    "    snp_cols = 4:(ncol(pheno_gt_base_pca)-12)\n",
    "    \n",
    "    snp_data = setupSNP(pheno_gt_base_pca, colSNPs=snp_cols, sep=\"/\")\n",
    "    \n",
    "    pca_cols = (ncol(pheno_gt_base_pca)-11):ncol(pheno_gt_base_pca)\n",
    "    pca_data = pheno_gt_base_pca[,pca_cols]\n",
    "    \n",
    "    wg_mass_co = WGassociation(Mass~1+pca_data$PC1+pca_data$PC2+pca_data$PC3+pca_data$PC4+pca_data$PC5+pca_data$PC6+pca_data$PC7+pca_data$PC8+pca_data$PC9+pca_data$PC10+pca_data$PC11+pca_data$PC12, data=snp_data, model=\"co\", genotypingRate=50)\n",
    "    \n",
    "    wg_pd_co = WGassociation(Pupual_Duration~1+pca_data$PC1+pca_data$PC2+pca_data$PC3+pca_data$PC4+pca_data$PC5+pca_data$PC6+pca_data$PC7+pca_data$PC8+pca_data$PC9+pca_data$PC10+pca_data$PC11+pca_data$PC12, data=snp_data, model=\"co\", genotypingRate=50)\n",
    "    \n",
    "    wg_tdt_co = WGassociation(Total_Dev_Time~1+pca_data$PC1+pca_data$PC2+pca_data$PC3+pca_data$PC4+pca_data$PC5+pca_data$PC6+pca_data$PC7+pca_data$PC8+pca_data$PC9+pca_data$PC10+pca_data$PC11+pca_data$PC12, data=snp_data, model=\"co\", genotypingRate=50)\n",
    "    \n",
    "    saveRDS(wg_mass_co, \"wg_mass_co.rds\")\n",
    "    ...\n",
    "    \n",
    "    wgstats_mass = WGstats(wg_mass_co)    \n",
    "    saveRDS(wgstats_mass, \"wgstats_mass.rds\")\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "wg_mass_co = readRDS(\"wg_mass_co.rds\")\n",
    "wg_pd_co = readRDS(\"wg_pd_co.rds\")\n",
    "wg_tdt_co = readRDS(\"wg_tdt_co.rds\")\n",
    "\n",
    "wgstats_mass = readRDS(\"wgstats_mass.rds\")\n",
    "wgstats_pd = readRDS(\"wgstats_pd.rds\")\n",
    "wgstats_tdt = readRDS(\"wgstats_tdt.rds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wgstats_mass = r['wgstats_mass']\n",
    "wgstats_mass_labels = r('labels(wg_mass_co)')\n",
    "\n",
    "wgstats_pd = r['wgstats_pd']\n",
    "wgstats_pd_labels = r('labels(wg_pd_co)')\n",
    "\n",
    "wgstats_tdt = r['wgstats_tdt']\n",
    "wgstats_tdt_labels = r('labels(wg_tdt_co)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = com.convert_robj(wgstats_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in test:\n",
    "    print pd.DataFrame(test[x])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_vals = {}\n",
    "\n",
    "wgstats = {\"mass\":[wgstats_mass, wgstats_mass_labels],\n",
    "           \"pd\":[wgstats_pd, wgstats_pd_labels],\n",
    "           \"tdt\":[wgstats_tdt, wgstats_tdt_labels]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key, datalist in wgstats.items():\n",
    "    print \"converting %s\" % key\n",
    "    wgstats[key] = [com.convert_robj(x) for x in datalist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_alleles(data):\n",
    "    a = set()\n",
    "    for x in data.index:\n",
    "        for elem in x.split(\"/\"):\n",
    "            a.add(elem)\n",
    "    return list(a)  \n",
    "\n",
    "def get_allele_freqs_wg(data, AA, Aa, aa):\n",
    "    total = np.sum(data['n'])*2\n",
    "    A = data.ix[AA, \"n\"]*2 + data.ix[Aa, \"n\"]\n",
    "    a = data.ix[aa, \"n\"]*2 + data.ix[Aa, \"n\"]\n",
    "    return A/total, a/total\n",
    "\n",
    "def get_genotypes(data, alleles):\n",
    "    homos = [\"%s/%s\" % (x,x) for x in alleles]\n",
    "    Aa = \"%s/%s\" % (alleles[0], alleles[1])\n",
    "    if Aa not in data.index:\n",
    "        Aa = Aa[::-1] #reverse it\n",
    "    AA, aa = homos\n",
    "    if data.ix[AA, \"n\"] < data.ix[aa, \"n\"]:\n",
    "        AA, aa = homos[::-1] #reverse it so that major is first\n",
    "    return AA, Aa, aa\n",
    "\n",
    "def get_genotypic_values(data, alleles):\n",
    "    AA, Aa, aa = get_genotypes(data, alleles)\n",
    "    G_AA = float(data.ix[AA, 'me'])\n",
    "    G_aa = float(data.ix[aa, 'me'])\n",
    "    additive = (G_AA-G_aa)/2\n",
    "    G_Aa = float(data.ix[Aa, 'me'])\n",
    "    dominance = G_Aa - ((G_AA+G_aa)/2)\n",
    "    return additive, dominance, AA, Aa, aa\n",
    "    \n",
    "def get_alpha(data):\n",
    "    alleles = get_alleles(data)\n",
    "    additive, dominance, AA, Aa, aa = get_genotypic_values(data, alleles)\n",
    "    p, q = get_allele_freqs_wg(data, AA, Aa, aa)\n",
    "    return additive + (dominance*(q-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lt3 = {}\n",
    "errors = {}\n",
    "for p in wgstats:\n",
    "    print \"running %s\" % p\n",
    "    df = pd.DataFrame(index=[\"alpha\", \"p\"])\n",
    "    alpha_vals[p] = df\n",
    "    lt3[p] = 0\n",
    "    errors[p] = set()\n",
    "    d = wgstats[p][0]\n",
    "    labels = wgstats[p][1]\n",
    "    for i, locus in enumerate(d):\n",
    "        try:\n",
    "            data = pd.DataFrame(d[locus])\n",
    "            snp = labels[i]\n",
    "            genotypes = [g for g in data.index if \"/\" in g]\n",
    "            data = data.ix[genotypes,:]\n",
    "            pvalue = data['p-value'].dropna()[0]\n",
    "            if len(genotypes) == 3:\n",
    "                alpha = get_alpha(data)\n",
    "                df[snp] = [alpha,pvalue]\n",
    "            elif len(genotypes) < 3:\n",
    "                lt3[p] += 1\n",
    "        except Exception as e: #needed for genotypes that are skipped b/c of genotyping rate\n",
    "            errors[p].add(e.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_files = []\n",
    "for p in alpha_vals:\n",
    "    d = alpha_vals[p].T\n",
    "    print len(d), len(d[d['p'] < 0.05])\n",
    "    \n",
    "#     f = \"alpha_%s.txt\" % p\n",
    "#     alpha_files.append(os.path.abspath(f))\n",
    "#     d.to_csv(f,\n",
    "#              index=True,\n",
    "#              header=False,\n",
    "#              sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "squat_dir = \"/data7/eckertlab/src/PolygenicAdaptationCode/Scripts/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_squat_vars(pheno):\n",
    "    d = {\"gwas.data.file\":\"\",\n",
    "         \"freqs.file\":\"\",\n",
    "         \"env.var.data.files\":\"list()\",\n",
    "         \"match.pop.file\":\"\",\n",
    "         \"full.dataset.file\":\"\",\n",
    "         \"path\":\"squat/%s\" % pheno,\n",
    "         \"match.categories\":\"\",\n",
    "         \"match.bins\":\"list(seq(0,0.5,0.02), c(2), seq(0,1000,100))\",\n",
    "         \"cov.SNPs.per.cycle\":5000,\n",
    "         \"cov.cycles\":1,\n",
    "         \"null.phenos.per.cycle\":1000,\n",
    "         \"null.cycles\":1,\n",
    "         \"load.cov.mat\":\"F\",\n",
    "         \"sim.null\":\"T\",\n",
    "         \"check.allele.orientation\":\"T\"}\n",
    "    return ',\\n'.join(\"%s=%s\" % (key,val) for (key,val) in d.items())\n",
    "\n",
    "def create_squat_run_file(pheno):\n",
    "    if not os.path.exists(\"squat\"):\n",
    "        os.mkdir(\"squat\")\n",
    "    squat_file = os.path.join(\"squat\", \"squat_%s.r\" % pheno)\n",
    "    with open(squat_file, \"w\") as o:\n",
    "        o.write(\"source(%s)\\n\" % os.path.join(squat_dir, \"funtions.R\"))\n",
    "        o.write(\"PolygenicAdaptationFunction(%s)\\n\" % get_squat_vars(pheno))\n",
    "    return squat_file\n",
    "\n",
    "for f in alpha_files:\n",
    "    pheno = os.path.basename(f).split(\".\")[0].split(\"_\")[1]\n",
    "    squat_file = create_squat_run_file(pheno)\n",
    "    print squat_file\n",
    "    !cat $squat_file\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
