{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import scandir\n",
    "import os\n",
    "import rpy2\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "import rpy2.robjects as ro\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import dill\n",
    "import random\n",
    "import vcf\n",
    "from hdfstorehelper import HDFStoreHelper\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import operator\n",
    "import traceback\n",
    "%load_ext rpy2.ipython\n",
    "from rpy2.robjects import pandas2ri as p2r\n",
    "p2r.activate()\n",
    "r = ro.r\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analysis_dir_notimp = \"/home/cfriedline/eckertlab/gypsy_indiv/masked/analysis/samtools1.2_no_otis/notimputed/\"\n",
    "analysis_dir_imp = \"/home/cfriedline/eckertlab/gypsy_indiv/masked/analysis/samtools1.2_no_otis/beagle40/\"\n",
    "hdf_notimp = HDFStoreHelper(os.path.join(analysis_dir_notimp, \"isect.hd5\"))\n",
    "hdf_imp = HDFStoreHelper(os.path.join(analysis_dir_imp, \"isect.hd5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hdf_notimp['z12_swapped'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf_notimp['z12_df'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analysis_dir = [analysis_dir_notimp, analysis_dir_imp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs = [hdf_notimp, hdf_imp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gps = {'QC32':[47.2509807, -79.4060515],\n",
    "      'QC93': [46.9089631, -70.8061075],\n",
    "      'NC': [36.449125, -76.024672],\n",
    "      'NY': [42.897768, -74.094761],\n",
    "      'VA1': [38.657615, -77.463603],\n",
    "      'VA2': [38.857470, -77.695003]}\n",
    "gps_df = pd.DataFrame(gps).T\n",
    "gps_df.columns = ['lat','lon']\n",
    "\n",
    "latlon = pandas2ri.py2ri(gps_df[['lon', 'lat']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(sp)\n",
    "library(raster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raster = r(\"raster\")\n",
    "extract = r(\"extract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bioclim_dir = \"/home/cfriedline/eckertlab/bioclim\"\n",
    "bioclim = !ls {bioclim_dir}/*.bil\n",
    "bioclim = sorted(bioclim)\n",
    "bioclim_df = pd.DataFrame(gps_df)\n",
    "for b in bioclim:\n",
    "    rast = raster(b)\n",
    "    bio = os.path.basename(b).replace(\".bil\", \"\").replace(\"_\", \"\").upper()\n",
    "    vals = pd.DataFrame(pandas2ri.ri2py(extract(rast, latlon)))\n",
    "    vals.index = bioclim_df.index\n",
    "    vals.columns = [bio]\n",
    "    bioclim_df = bioclim_df.join(vals)\n",
    "bioclim_df = bioclim_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for hdf in hdfs:\n",
    "    hdf['bioclim'] = bioclim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "region_id = 1\n",
    "for popname in bioclim_df.index:\n",
    "    bioclim_df.ix[popname, 'region'] = region_id\n",
    "    region_id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bioclim_env_files = []\n",
    "for bio in bioclim_df[[x for x in bioclim_df if 'BIO' in x]]:\n",
    "    bio_temp = bioclim_df[[bio, 'region']]\n",
    "    bio_temp.index.name = \"CLST\"\n",
    "    bio_temp.columns = [\"ENV\", \"REG\"]\n",
    "    bio_out = os.path.join(analysis_dir[0], \"../%s.txt\" % bio)\n",
    "    bioclim_env_files.append(bio_out)\n",
    "    bio_temp.to_csv(bio_out, sep=\"\\t\", header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z12_swapped = [x['z12_swapped'] for x in hdfs]\n",
    "z12 = [x['z12_df'] for x in hdfs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def get_correction(n):\n",
    "    #for finite sample size\n",
    "    return (2*n)/(2*n-1)\n",
    "\n",
    "def get_allele_freqs(locus, debug):\n",
    "    c = locus[locus != -1].value_counts()\n",
    "    total_alleles = 2.0*sum(c)\n",
    "    num_individuals = sum(c)\n",
    "    P = 0\n",
    "    Q = 0\n",
    "    PQ = 0\n",
    "    if 0 in c:\n",
    "        P = 2*c[0]\n",
    "    if 2 in c:\n",
    "        Q = 2*c[2]\n",
    "    if 1 in c:\n",
    "        PQ = c[1]\n",
    "    P += PQ\n",
    "    Q += PQ\n",
    "    p = P/total_alleles\n",
    "    q = Q/total_alleles\n",
    "    assert p + q == 1.0\n",
    "    He = 2 * p * q * get_correction(num_individuals)\n",
    "    Ho = PQ*1.0/num_individuals\n",
    "    Fis = 1 - (Ho/He)\n",
    "    #print p, q, He, Ho, Fis\n",
    "    \n",
    "        \n",
    "    ret = pd.Series({\"p\":p, \n",
    "                      \"q\":q,\n",
    "                      \"P\":P,\n",
    "                      \"Q\":Q,\n",
    "                      \"He\":He,\n",
    "                      \"Ho\":Ho, \n",
    "                      \"Fis\":Fis})\n",
    "    if debug:\n",
    "        print(ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allele_freqs = [x.ix[:,:-1].apply(get_allele_freqs, args=(False,)) for x in z12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allele_freqs[0].shape, allele_freqs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, af in enumerate(allele_freqs):\n",
    "    hdfs[i]['allele_freqs_squat'] = af\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pop_allele_freqs = []\n",
    "for df in z12:\n",
    "    paf = {}\n",
    "    pop_allele_freqs.append(paf)\n",
    "    for pop, data in df.groupby('population'):\n",
    "        print(pop)\n",
    "        data = data.ix[:,:-1]\n",
    "        paf[pop] = data.apply(get_allele_freqs, args=(False,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, paf in enumerate(pop_allele_freqs):\n",
    "    outfile = os.path.join(analysis_dir[i], \"pop_allele_freqs.dill\")\n",
    "    print(outfile)\n",
    "    dill.dump(paf, open(outfile, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pop_allele_freqs = []\n",
    "for i, d in enumerate(analysis_dir):\n",
    "    dill_file = os.path.join(d, \"pop_allele_freqs.dill\")\n",
    "    pop_allele_freqs.append(dill.load(open(dill_file, \"rb\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pimass_gt = [x['pimass_gt'].replace(\"NA\", np.nan) for x in hdfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gt_ref_alt = [x['gt_ref_alt_df'] for x in hdfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gt_ref_alt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translation_df = pd.read_csv(\"translation_table.csv\", sep=\"\\t\", index_col=0)\n",
    "def get_translated_name(n):\n",
    "    n = n.strip()\n",
    "    if n in translation_df.index:\n",
    "        row = translation_df.ix[n.strip()]\n",
    "        return \"%s_%d_%d\" % (row['pop'], row.indiv, row.dup)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for p in pimass_gt:\n",
    "    p.columns = [get_translated_name(x) for x in p.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pimass_pheno = [x['pimass_pheno'] for x in hdfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pimass_pheno[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pp in pimass_pheno:\n",
    "    pp.columns = [x.lower().replace(\" \", \"_\") for x in pp.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phenos = [x[['mass_resid', 'pupual_duration_resid', 'total_dev_time_resid']] for x in pimass_pheno]\n",
    "phenos = [x[['massx', 'pdx', 'tdtx']] for x in pimass_pheno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alphas = []\n",
    "for i, pheno_df in enumerate(phenos):\n",
    "    alphas.append({})\n",
    "    data = pheno_df.join(pimass_gt[i].T.ix[2:,])\n",
    "    data.columns = [x.replace(\" \", \"_\") for x in data.columns]\n",
    "    data = data.astype(float)\n",
    "    for pheno in data.columns[0:3]:\n",
    "        alphas[i][pheno] = {}\n",
    "        print(i, pheno)\n",
    "        for snp in data.columns[3:]:\n",
    "            model = sm.OLS(data[pheno], data[snp], missing='drop')\n",
    "            fit = model.fit()\n",
    "            alphas[i][pheno][snp] = fit.params.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_df = [pd.DataFrame(x) for x in alphas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, ad in enumerate(alpha_df):\n",
    "    hdfs[i]['alpha_df'] = ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pheno_names = phenos[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pheno_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sig_pheno_name(col):\n",
    "    return {'mass_resid': 'mass',\n",
    "           'pupual_duration_resid': 'pd',\n",
    "           'total_dev_time_resid': 'tdt',\n",
    "           'massx': 'mass',\n",
    "           'tdtx': 'tdt',\n",
    "           'pdx':'pd'}[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_freqs_file(df, paf, outdir, pheno, imputed, key, sig_key):\n",
    "    df = df.copy()\n",
    "    outfile = os.path.join(outdir, \"freqs.file.%s.%s.%s.%s.txt\" % (sig_key, key, pheno, imputed))\n",
    "    df.index.name = \"SNP\"\n",
    "    dfs = []\n",
    "    for pop in paf:\n",
    "        df2 = df.join(paf[pop].T)\n",
    "        df2['CLST'] = pop\n",
    "        df2['POS'] = df2.apply(lambda x: x.name.split(\"_\")[1], axis=1)\n",
    "        df2['CHR'] = df2.apply(lambda x: x.name.split(\"_\")[0], axis=1)\n",
    "        df2 = df2[['CLST', 'ref', 'alt', 'p', 'POS', 'CHR']]\n",
    "        df2.columns = ['CLST', 'A1', 'A2', 'FRQ', 'POS', 'CHR']\n",
    "        dfs.append(df2)\n",
    "    combined = pd.concat(dfs)\n",
    "    combined.to_csv(outfile, header=True, index=True, sep=\"\\t\")\n",
    "    return outfile, combined['CLST'].unique()\n",
    "\n",
    "def write_gwas_data_file(df, outdir, pheno, imputed, sig_key):\n",
    "    df = df.copy()\n",
    "    outfile = os.path.join(outdir, \"gwas.data.file.%s.%s.%s.txt\" % (sig_key, pheno, imputed))\n",
    "    df.index.name = \"SNP\"\n",
    "    df = df[['ref', 'alt', 'alpha', 'frq']]\n",
    "    df.columns = [\"A1\", \"A2\", \"EFF\", \"FRQ\"]\n",
    "    df.to_csv(outfile, header=True, index=True, sep=\"\\t\")\n",
    "    return outfile\n",
    "    \n",
    "def write_env_var_data_files(paf, pops, outdir, imputed, pheno, sig_key):\n",
    "    outfiles = []\n",
    "    outfile = os.path.join(outdir, \"env.var.data.file.%s.%s.%s.txt\" % (sig_key, pheno, imputed))\n",
    "    outfiles.append(outfile)\n",
    "    with open(outfile, \"w\") as o:\n",
    "        o.write(\"CLST\\tENV\\tREG\\n\")\n",
    "        pop_id = 1\n",
    "        for pop in sorted(paf):\n",
    "            if pop in pops:\n",
    "                o.write(\"%s\\t%g\\t%d\\n\" % (pop, np.random.randn(), pop_id))\n",
    "            pop_id += 1\n",
    "    return outfiles\n",
    "  \n",
    "def write_match_pop_file(df, paf, outdir, imputed, matchpop, pheno, sig_key):\n",
    "    outfile = os.path.join(outdir, \"match.pop.file.%s.%s.%s.%s.txt\" % (sig_key, pheno, matchpop, imputed))\n",
    "    #df2 = df.join(paf[matchpop].T)\n",
    "    df2 = df.copy()\n",
    "    df2.index.name=\"SNP\"\n",
    "    df2['CLST'] = matchpop\n",
    "    df2['POS'] = df2.apply(lambda x: x.name.split(\"_\")[1], axis=1)\n",
    "    df2['CHR'] = df2.apply(lambda x: x.name.split(\"_\")[0], axis=1)\n",
    "    df2 = df2[['CLST', 'ref', 'alt', 'frq', 'POS', 'CHR']]\n",
    "    df2.columns = ['CLST', 'A1', 'A2', 'FRQ', 'POS', 'CHR']\n",
    "    df2.to_csv(outfile, header=True, index=True, sep=\"\\t\")\n",
    "    return outfile\n",
    "\n",
    "def get_squat_vars(pheno,\n",
    "                   gwas_data_file, \n",
    "                  gwas_freqs_file, \n",
    "                  env_var_files,\n",
    "                  match_pop_file,\n",
    "                  full_freqs_file,\n",
    "                  num_snps):\n",
    "    d = {\"gwas.data.file\": \"'%s'\" % gwas_data_file,\n",
    "         \"freqs.file\": \"'%s'\" % gwas_freqs_file,\n",
    "         \"env.var.data.files\": \"list(%s)\" % ','.join([\"'%s'\" % x for x in env_var_files]),\n",
    "         \"match.pop.file\": \"'%s'\" % match_pop_file,\n",
    "         \"full.dataset.file\": \"'%s'\" % full_freqs_file,\n",
    "         \"path\":\"'%s'\" % pheno,\n",
    "         \"match.categories\":\"c('MAF')\",\n",
    "         \"match.bins\":\"list(seq(0,0.5,0.005))\",\n",
    "         \"cov.SNPs.per.cycle\":10000,\n",
    "         \"cov.cycles\":10,\n",
    "         \"null.phenos.per.cycle\":1000,\n",
    "         \"null.cycles\":10,\n",
    "         \"load.cov.mat\":\"F\",\n",
    "         \"sim.null\":\"T\",\n",
    "         \"check.allele.orientation\":\"T\"}\n",
    "    return ',\\n'.join(\"%s=%s\" % (key,val) for (key,val) in list(d.items()))\n",
    "\n",
    "def create_squat_run_file(pheno, outdir, squat_vars, sig_key):\n",
    "    squat_dir = \"/home/cfriedline/eckertlab/src/PolygenicAdaptationCode/Scripts/\"\n",
    "    out_dir = os.path.join(outdir, \"squat_%s\" % sig_key)\n",
    "    res_dir = os.path.join(out_dir, pheno)\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.mkdir(out_dir)\n",
    "        os.symlink(squat_dir, os.path.join(out_dir, \"Scripts\"))\n",
    "    squat_file = os.path.join(out_dir, \"squat_%s.%s.r\" % (sig_key, pheno))\n",
    "    with open(squat_file, \"w\") as o:\n",
    "        o.write(\"system('rm -rf %s')\\n\" % res_dir)\n",
    "        o.write(\"setwd('/home/cfriedline/eckertlab/src/PolygenicAdaptationCode')\\n\")\n",
    "        o.write(\"source('%s')\\n\" % os.path.join(squat_dir, \"CreateTraitFile.R\"))\n",
    "        o.write(\"source('%s')\\n\" % os.path.join(squat_dir, \"functions.R\"))\n",
    "        o.write(\"setwd('%s')\\n\" % out_dir)\n",
    "        o.write(\"PolygenicAdaptationFunction(%s)\\n\" % squat_vars)\n",
    "    return squat_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "squat_files = []\n",
    "\n",
    "for i, hdf in enumerate(hdfs):\n",
    "    outdir = analysis_dir[i]\n",
    "    for name in pheno_names:\n",
    "        paf = pop_allele_freqs[i]\n",
    "        pheno_hdf = get_sig_pheno_name(name)\n",
    "        for sig_key, sig in list({\"relaxed\": hdf['relaxed_sig_snps_%s' % pheno_hdf],\n",
    "                            \"sig\": hdf['sig_snps_%s' % pheno_hdf]}.items()):\n",
    "            print(sig_key, name, pheno_hdf, len(sig))\n",
    "            full = hdf['mcmc_%s_hmean' % pheno_hdf]\n",
    "            full.index = full.rs\n",
    "            full.index.name=\"%s_%d\" % (name, i)\n",
    "            alpha = alphas[i][name]\n",
    "            full['alpha'] = full.apply(lambda x: alpha[x.name], axis=1)       \n",
    "            full = full.drop(\"rs\", axis=1)\n",
    "            full = full.join(gt_ref_alt[i].T[['ref', 'alt']])\n",
    "            af = hdfs[i]['allele_freqs'] #this is ref, alt.  ref will be A1, `p` is its freq\n",
    "            full['frq'] = full.apply(lambda x: af[x.name]['p'], axis=1)\n",
    "\n",
    "            # write files\n",
    "            gwas_data_file = write_gwas_data_file(full.ix[sig.rs], outdir, name, i, sig_key)\n",
    "            gwas_freqs_file, gwas_pops = write_freqs_file(full.ix[sig.rs], paf, outdir, name, i, \"gwas\", sig_key)\n",
    "            #env_var_files = write_env_var_data_files(paf, gwas_pops, outdir, i, name, sig_key)\n",
    "            match_pop_file = write_match_pop_file(full, paf, outdir, i, \"ALL\", name, sig_key)\n",
    "            full_freqs_file, full_pops = write_freqs_file(full, paf, outdir, name, i, \"full\", sig_key)\n",
    "            squat_vars = get_squat_vars(name, gwas_data_file,\n",
    "                          gwas_freqs_file, \n",
    "                          bioclim_env_files,\n",
    "                          match_pop_file,\n",
    "                          full_freqs_file, len(full))\n",
    "            squat_file = create_squat_run_file(name, outdir, squat_vars, sig_key)\n",
    "            print(\"wrote %s\" % squat_file)\n",
    "            squat_files.append(squat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(os.path.commonprefix(squat_files), \"squat_parallel.txt\"), \"w\") as o:\n",
    "    print(o.name)\n",
    "    for rsf in squat_files:\n",
    "        o.write(\"cd %s && R --slave --vanilla < %s > /dev/null\\n\" % (os.path.dirname(rsf), rsf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## create squat files for range snps\n",
    "range_squat_files = []\n",
    "for i, hdf in enumerate(hdfs):\n",
    "    if i == 0:\n",
    "        pass\n",
    "    outdir = analysis_dir[i]\n",
    "    for name in pheno_names:\n",
    "        paf = pop_allele_freqs[i]\n",
    "        pheno_hdf = get_sig_pheno_name(name)\n",
    "        snp_ranges = hdf['%s_quant_range' % pheno_hdf]\n",
    "        full = hdf['mcmc_%s_hmean' % pheno_hdf]\n",
    "        full.index = full.rs\n",
    "        full.index.name=\"%s_%d\" % (name, i)\n",
    "        alpha = alphas[i][name]\n",
    "        full['alpha'] = full.apply(lambda x: alpha[x.name], axis=1)       \n",
    "        full = full.drop(\"rs\", axis=1)\n",
    "        \n",
    "        full = full.join(gt_ref_alt[i].T[['ref', 'alt']])\n",
    "        af = hdfs[i]['allele_freqs'] #this is ref, alt.  ref will be A1, `p` is its freq\n",
    "        full['frq'] = full.apply(lambda x: af[x.name]['p'], axis=1)\n",
    "        \n",
    "        try:\n",
    "            for j, val in enumerate(snp_ranges):\n",
    "                exp_dir = os.path.join(outdir, \"squat_ranges\")\n",
    "                exp_dir = os.path.join(exp_dir, \"%s_range_%d\" % (pheno_hdf, j))\n",
    "                if not os.path.exists(exp_dir):\n",
    "                    os.makedirs(exp_dir)\n",
    "                else:\n",
    "                    shutil.rmtree(exp_dir)\n",
    "                    os.makedirs(exp_dir)\n",
    "                sig = hdf['range_snps_%s_%d' % (pheno_hdf, j)]\n",
    "                sig_key = \"range_%d\" % j\n",
    "                gwas_data_file = write_gwas_data_file(full.ix[sig.rs], exp_dir, name, i, sig_key)\n",
    "                gwas_freqs_file, gwas_pops = write_freqs_file(full.ix[sig.rs], paf, exp_dir, name, i, \"gwas\", sig_key)\n",
    "                match_pop_file = write_match_pop_file(full, paf, exp_dir, i, \"ALL\", name, sig_key)\n",
    "                full_freqs_file, full_pops = write_freqs_file(full, paf, exp_dir, name, i, \"full\", sig_key)\n",
    "                squat_vars = get_squat_vars(name, gwas_data_file,\n",
    "                              gwas_freqs_file, \n",
    "                              bioclim_env_files,\n",
    "                              match_pop_file,\n",
    "                              full_freqs_file, len(full))\n",
    "                squat_file = create_squat_run_file(name, exp_dir, squat_vars, sig_key)\n",
    "                print(\"wrote %s\" % squat_file)\n",
    "                range_squat_files.append(squat_file)\n",
    "        except TypeError as e:\n",
    "            pass\n",
    "        except IOError as e: \n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ni = open(\"/home/cfriedline/eckertlab/gypsy_indiv/masked/analysis/samtools1.2_no_otis/notimputed/squat_parallel.txt\", \"w\")\n",
    "bg = open(\"/home/cfriedline/eckertlab/gypsy_indiv/masked/analysis/samtools1.2_no_otis/beagle40/squat_parallel.txt\", \"w\")\n",
    "ni_files = []\n",
    "bg_files = []\n",
    "for rsf in range_squat_files:\n",
    "    o = ni\n",
    "    if 'beagle40' in rsf:\n",
    "        o = bg\n",
    "        bg_files.append(rsf)\n",
    "    else:\n",
    "        ni_files.append(rsf)\n",
    "    o.write(\"cd %s && R --slave --vanilla < %s > /dev/null\\n\" % (os.path.dirname(rsf), rsf))\n",
    "[x.close() for x in [ni, bg]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "~/bin/parallel --no-notice --bar -a squat_parallel.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_df[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mcmc_mass = hdfs[1]['mcmc_mass_hmean']\n",
    "mcmc_mass.index = mcmc_mass.rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(alpha_df[1].massx)\n",
    "plt.show()\n",
    "sns.distplot(alpha_df[1].pdx)\n",
    "plt.show()\n",
    "sns.distplot(alpha_df[1].tdtx)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "range_outputs = !find {os.path.commonprefix(range_squat_files)} | grep 'Output$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "range_outputs = !find {os.path.commonprefix(bg_files)} | grep 'Output$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "range_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "range_output_data = []\n",
    "used_outputs = []\n",
    "for output in range_outputs:\n",
    "    pheno_range = os.path.basename(os.path.dirname(os.path.dirname(os.path.dirname(output))))\n",
    "    pheno_data = pheno_range.split(\"_\")\n",
    "    pheno = pheno_data[0]\n",
    "    range_id = int(pheno_data[-1])\n",
    "    ranges = hdf['%s_quant_range' % pheno]\n",
    "    r(\"rm(list=ls())\")\n",
    "    for obj in os.listdir(output):\n",
    "        obj = os.path.join(output, obj)\n",
    "        r(\"load('%s')\" % obj)\n",
    "    try:\n",
    "        range_output_data.append({'pheno': pheno,\n",
    "                                'pheno_range': pheno_range,\n",
    "                                      'cutoff':ranges[range_id],\n",
    "                                          'Qx': r('the.stats$Qx')[0],\n",
    "                                          'Pr(Qx)': r('p.vals$Qx')[0],\n",
    "                                          'Fst': r('the.stats$Fst.comp')[0],\n",
    "                                          'Pr(Fst)': r('p.vals$Fst.comp')[0],\n",
    "                                          'LD': r('the.stats$LD.comp')[0],\n",
    "                                          'Pr(LD)': r('p.vals$LD.comp')[0]})\n",
    "        used_outputs.append(output)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_gwas = {}\n",
    "for uo in used_outputs:\n",
    "    res = !wc -l {os.path.dirname(os.path.dirname(os.path.dirname(uo)))}/gwas*.txt\n",
    "    res = res[0].split()\n",
    "    n = res[0]\n",
    "    p = os.path.basename(os.path.dirname(res[1]))\n",
    "    num_gwas[p] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "range_output_data = []\n",
    "for output in range_outputs:\n",
    "    pheno_range = os.path.basename(os.path.dirname(os.path.dirname(os.path.dirname(output))))\n",
    "    pheno_data = pheno_range.split(\"_\")\n",
    "    pheno = pheno_data[0]\n",
    "    range_id = int(pheno_data[-1])\n",
    "    if range_id ==  0:\n",
    "        ranges = hdf['%s_quant_range' % pheno]\n",
    "        r(\"rm(list=ls())\")\n",
    "        for obj in os.listdir(output):\n",
    "            obj = os.path.join(output, obj)\n",
    "            print(obj)\n",
    "            r(\"load('%s')\" % obj)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "hist(null.stats$Qx, xlim=c(0, the.stats$Qx+50))\n",
    "abline(v=the.stats$Qx, col=\"red\")\n",
    "text(150, 1000, \"Qx\", col=\"red\")\n",
    "hist(null.stats$LD.component, xlim=c(-5, the.stats$LD.component+50))\n",
    "abline(v=the.stats$LD.component, col=\"red\")\n",
    "text(150, 1000, \"LD\", col=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "range_output_df = pd.DataFrame(range_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "range_output_df['num'] = range_output_df.apply(lambda x: num_gwas[x.pheno_range], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "range_output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "sns.set_context(\"talk\")\n",
    "for group, data in range_output_df.groupby(\"pheno\"):\n",
    "    data = data.sort_values(\"cutoff\", ascending=False)\n",
    "    print(len(data))\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(30,5)\n",
    "    ax1 = fig.add_subplot(131)\n",
    "    ax1.plot(data.cutoff, data.Qx, color=\"blue\")\n",
    "    plt.ylabel(\"Qx\")\n",
    "    plt.xlabel(\"PIP cutoff\")\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(data.cutoff, data['Pr(Qx)'], color=\"red\")\n",
    "    plt.ylabel(\"Pr(Qx)\")\n",
    "    ax2.grid(None)\n",
    "    plt.title(group)\n",
    "    red_patch = mpatches.Patch(color='red', label='Pr(Qx)')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Qx')\n",
    "    for row in data.index:\n",
    "        ax1.annotate(data.ix[row,'num'], \n",
    "                     (data.ix[row, 'cutoff'], data.ix[row, 'Qx']),\n",
    "                    xytext = (-1, 1),\n",
    "                    textcoords = 'offset points',\n",
    "                    ha = 'right', va = 'bottom',\n",
    "                    bbox = dict(boxstyle = 'round,pad=0.5', fc = 'yellow', alpha = 0.5))\n",
    "    \n",
    "    plt.legend(handles=[blue_patch, red_patch], loc=\"upper center\")\n",
    "    \n",
    "    ax3 = plt.subplot(132)\n",
    "    ax3.plot(data.cutoff, data.LD, color=\"blue\")\n",
    "    plt.ylabel(\"LD\")\n",
    "    ax4 = ax3.twinx()\n",
    "    ax4.plot(data.cutoff, data['Pr(LD)'], color=\"red\")\n",
    "    plt.ylabel(\"Pr(LD)\")\n",
    "    ax4.grid(None)\n",
    "    red_patch = mpatches.Patch(color='red', label='Pr(LD)')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='LD')\n",
    "    plt.legend(handles=[blue_patch, red_patch], loc=\"upper center\")\n",
    "    plt.title(group)\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    \n",
    "    ax5 = plt.subplot(133)\n",
    "    ax5.plot(data.cutoff, data.Fst, color=\"blue\")\n",
    "    plt.ylabel(\"Fst\")\n",
    "    ax6 = ax5.twinx()\n",
    "    ax6.plot(data.cutoff, data['Pr(Fst)'], color=\"red\")\n",
    "    plt.ylabel(\"Pr(Fst)\")\n",
    "    ax6.grid(None)\n",
    "    red_patch = mpatches.Patch(color='red', label='Pr(Fst)')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Fst')\n",
    "    plt.legend(handles=[blue_patch, red_patch], loc=\"upper center\")\n",
    "    plt.title(group)\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "squat_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "setwd(\"~/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for squat_file in squat_files:\n",
    "    print(squat_file)\n",
    "    r(\"source('%s')\" % squat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(os.path.dirname(os.path.dirname(analysis_dir_imp)), \"squat_results.txt\"), \"w\") as o:  \n",
    "    o.write(\"%s\\n\" % \"\\t\".join([\"significance\", \"phenotype\", \"imputation\", \"Qx\", \"Pr(Qx)\", \n",
    "                               \"Fst\", \"Pr(Fst)\", \"LD\", \"Pr(LD)\"]))\n",
    "    for squat_file in squat_files:\n",
    "        squat_dir = os.path.dirname(squat_file)\n",
    "        datatype = os.path.basename(os.path.dirname(squat_dir))\n",
    "        pheno = os.path.basename(squat_file).split(\".\")[1]\n",
    "        pheno_dir = os.path.join(squat_dir, pheno)\n",
    "        output_dir = os.path.join(pheno_dir, \"Output\")\n",
    "        assert os.path.exists(output_dir)\n",
    "\n",
    "        for obj in os.listdir(output_dir):\n",
    "            obj = os.path.join(output_dir, obj)\n",
    "            r(\"load('%s')\" % obj)\n",
    "            \n",
    "        res = \"\\t\".join([str(x) for x in [os.path.basename(squat_dir),\n",
    "                                          pheno,\n",
    "                                          datatype,\n",
    "                                          r('the.stats$Qx')[0],\n",
    "                                          r('p.vals$Qx')[0],\n",
    "                                          r('the.stats$Fst.comp')[0],\n",
    "                                          r('p.vals$Fst.comp')[0],\n",
    "                                          r('the.stats$LD.comp')[0],\n",
    "                                          r('p.vals$LD.comp')[0]]])\n",
    "        print(res)\n",
    "        o.write(\"%s\\n\" % res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
