{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, vcf, gzip\n",
    "from IPython.parallel import Client\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from __future__ import division\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# R_HOME = \"/Library/Frameworks/R.framework/Resources\"\n",
    "os.environ['R_HOME'] = R_HOME\n",
    "# os.environ['LD_LIBRARY_PATH'] = \"%s/lib\" % os.environ['R_HOME']\n",
    "os.environ['LD_LIBRARY_PATH'] = \"%s/lib:%s\" % (os.environ['R_HOME'], os.environ['LD_LIBRARY_PATH'])\n",
    "import rpy2.robjects as robjects\n",
    "%load_ext rpy2.ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas.rpy.common as com\n",
    "import scipy.stats as stats\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from multiprocessing import Pool\n",
    "from scipy.stats.stats import pearsonr\n",
    "import numbers\n",
    "import warnings\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warnings.simplefilter(action=\"ignore\", category=pd.io.pytables.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_views():\n",
    "#     rc = Client(profile='huge')\n",
    "#     rc = Client(profile='default')\n",
    "    rc = Client(profile='sge')\n",
    "    dview = rc[:]\n",
    "    lview = rc.load_balanced_view()\n",
    "    print \"Connected to %d IPython engines\" % len(rc)\n",
    "    return rc, dview, lview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rc, dview, lview = get_views()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rc, dview, lview = get_views()\n",
    "dview['R_HOME'] = R_HOME\n",
    "with dview.sync_imports():\n",
    "    import os\n",
    "    import sys\n",
    "    import scipy.stats\n",
    "    import statsmodels.formula.api\n",
    "    import statsmodels.api\n",
    "    from scipy.stats.stats import pearsonr\n",
    "    import numpy\n",
    "    import scipy\n",
    "    import pandas\n",
    "    import rpy2\n",
    "    import numbers\n",
    "    import traceback\n",
    "    import socket\n",
    "    import pymongo\n",
    "    \n",
    "def setup_engines():   \n",
    "    os.environ['R_HOME'] = R_HOME\n",
    "    os.environ['LD_LIBRARY_PATH'] = \"%s/lib:%s\" % (os.environ['R_HOME'], os.environ['LD_LIBRARY_PATH'])\n",
    "#     os.environ['LD_LIBRARY_PATH'] = \"%s/lib\" % (os.environ['R_HOME'])\n",
    "    import rpy2.robjects as robjects\n",
    "    r = robjects.r\n",
    "    r(\"source('tw_calc.R')\")\n",
    "    r(\"test=read.table('twtable', header=F)\")\n",
    "\n",
    "dview['setup_engines'] = setup_engines\n",
    "setup = dview.apply(setup_engines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vcftools = \"~/data7/src/vcftools_0.1.12a/bin/vcftools\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    /home/cfriedline/data7/src/freebayes/bin/freebayes -L /data7/eckertlab/gypsy_indiv/bamlist.txt -f /home/cfriedline/data7/assemblies/gypsy/masurca/CA/10-gapclose/genome.ctg.fasta -v /data7/eckertlab/gypsy_indiv/gypsy_freebayes_out.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vcf called from freebayes\n",
    "fb_vcf = \"/data7/eckertlab/gypsy_indiv/gypsy_freebayes_out.vcf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!$vcftools --vcf $fb_vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    VCFtools - v0.1.12a\n",
    "    (C) Adam Auton and Anthony Marcketta 2009\n",
    "\n",
    "    Parameters as interpreted:\n",
    "        --vcf /data7/eckertlab/gypsy_indiv/gypsy_freebayes_out.vcf\n",
    "\n",
    "    After filtering, kept 192 out of 192 Individuals\n",
    "    After filtering, kept 86785 out of a possible 86785 Sites\n",
    "    Run Time = 3.00 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Outputs the allele frequency for each site in a file with the suffix \".frq\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!$vcftools --freq --remove-indels --vcf $fb_vcf --out $fb_vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    VCFtools - v0.1.12a\n",
    "    (C) Adam Auton and Anthony Marcketta 2009\n",
    "\n",
    "    Parameters as interpreted:\n",
    "        --vcf /data7/eckertlab/gypsy_indiv/gypsy_freebayes_out.vcf\n",
    "        --freq\n",
    "        --out /data7/eckertlab/gypsy_indiv/gypsy_freebayes_out.vcf\n",
    "        --remove-indels\n",
    "\n",
    "    After filtering, kept 192 out of 192 Individuals\n",
    "    Outputting Frequency Statistics...\n",
    "    After filtering, kept 79761 out of a possible 86785 Sites\n",
    "    Run Time = 43.00 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Outputs the raw allele counts for each site in a file with the suffix \".frq.count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!$vcftools --counts --remove-indels --vcf $fb_vcf --out $fb_vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    VCFtools - v0.1.12a\n",
    "    (C) Adam Auton and Anthony Marcketta 2009\n",
    "\n",
    "    Parameters as interpreted:\n",
    "        --vcf /data7/eckertlab/gypsy_indiv/gypsy_freebayes_out.vcf\n",
    "        --counts\n",
    "        --out /data7/eckertlab/gypsy_indiv/gypsy_freebayes_out.vcf\n",
    "        --remove-indels\n",
    "\n",
    "    After filtering, kept 192 out of 192 Individuals\n",
    "    Outputting Frequency Statistics...\n",
    "    After filtering, kept 79761 out of a possible 86785 Sites\n",
    "    Run Time = 41.00 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Calculates a measure of heterozygosity on a per-individual basis. Specfically, the inbreeding coefficient, F, is estimated for each individual using a method of moments. The resulting file has the suffix \".het\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!$vcftools --het --remove-indels --vcf $fb_vcf --out $fb_vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    VCFtools - v0.1.12a\n",
    "    (C) Adam Auton and Anthony Marcketta 2009\n",
    "\n",
    "    Parameters as interpreted:\n",
    "        --vcf /data7/eckertlab/gypsy_indiv/gypsy_freebayes_out.vcf\n",
    "        --het\n",
    "        --out /data7/eckertlab/gypsy_indiv/gypsy_freebayes_out.vcf\n",
    "        --remove-indels\n",
    "\n",
    "    After filtering, kept 192 out of 192 Individuals\n",
    "    Outputting Individual Heterozygosity\n",
    "        Individual Heterozygosity: Only using fully diploid SNPs.\n",
    "        Individual Heterozygosity: Only using biallelic SNPs.\n",
    "    After filtering, kept 79761 out of a possible 86785 Sites\n",
    "    Run Time = 40.00 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Reports a p-value for each site from a Hardy-Weinberg Equilibrium test (as defined by Wigginton, Cutler and Abecasis (2005)). The resulting file (with suffix \".hwe\") also contains the Observed numbers of Homozygotes and Heterozygotes and the corresponding Expected numbers under HWE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!$vcftools --hardy --vcf $fb_vcf --out $fb_vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "This option outputs the genotypes as a large matrix. Three files are produced. The first, with suffix \".012\", contains the genotypes of each individual on a separate line. Genotypes are represented as 0, 1 and 2, where the number represent that number of non-reference alleles. Missing genotypes are represented by -1. The second file, with suffix \".012.indv\" details the individuals included in the main file. The third file, with suffix \".012.pos\" details the site locations included in the main file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!$vcftools --remove-indels --min-alleles 2 --max-alleles 2 --mac 1 --remove-filtered-all --012 --vcf $fb_vcf --out $fb_vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    VCFtools - v0.1.12a\n",
    "    (C) Adam Auton and Anthony Marcketta 2009\n",
    "\n",
    "    Parameters as interpreted:\n",
    "        --vcf /data7/eckertlab/gypsy_indiv/gypsy_freebayes_out.vcf\n",
    "        --mac 1\n",
    "        --max-alleles 2\n",
    "        --min-alleles 2\n",
    "        --012\n",
    "        --out /data7/eckertlab/gypsy_indiv/gypsy_freebayes_out.vcf\n",
    "        --remove-filtered-all\n",
    "        --remove-indels\n",
    "\n",
    "    After filtering, kept 192 out of 192 Individuals\n",
    "    Writing 012 matrix files ... Done.\n",
    "    After filtering, kept 30791 out of a possible 86785 Sites\n",
    "    Run Time = 55.00 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snp_quals = []\n",
    "snp_dp = []\n",
    "for i, rec in enumerate(vcf.Reader(open(fb_vcf))):\n",
    "    if rec.INFO['TYPE'][0] == 'snp':\n",
    "        snp_quals.append(rec.QUAL)\n",
    "        snp_dp.append(rec.INFO['DP'])\n",
    "    if i % 5000 == 0:\n",
    "        print \"at %d\" % i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(snp_quals, bins=100)\n",
    "plt.title(\"%.2f +/- %.2f [%d, %d]\" % (np.mean(snp_quals), np.std(snp_quals), np.min(snp_quals), np.max(snp_quals)))\n",
    "plt.xlabel(\"QUAL\")\n",
    "plt.xlim(0, 10000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(snp_dp, bins=100)\n",
    "plt.title(\"%.2f +/- %.2f [%d, %d]\" % (np.mean(snp_dp), np.std(snp_dp), np.min(snp_dp), np.max(snp_dp)))\n",
    "plt.xlabel(\"DP\")\n",
    "plt.xlim((0,700))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "##Create file for afsource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_missing(rec):\n",
    "    missing = 0.0\n",
    "    for sample in rec.samples:\n",
    "        if not sample.data.AO:\n",
    "            missing += 1\n",
    "    return missing\n",
    "\n",
    "def is_valid_snp(rec, min_qual, missing_prob):\n",
    "    if rec.QUAL < min_qual:\n",
    "        return False\n",
    "    t = rec.INFO['TYPE']\n",
    "    if len(t) > 1:\n",
    "        return False\n",
    "    if t[0] != 'snp':\n",
    "        return False\n",
    "    if get_missing(rec)/len(rec.samples) > missing_prob:\n",
    "        return False\n",
    "    return True    \n",
    "\n",
    "min_qual = 0\n",
    "missing_prob = 1.0\n",
    "afsource_file = \"%s_qual_%d_missing_%.2f.afsource\" % (fb_vcf, min_qual, missing_prob)\n",
    "with open(afsource_file, \"w\") as o:\n",
    "    for i, rec in enumerate(vcf.Reader(open(fb_vcf))):\n",
    "        if is_valid_snp(rec, min_qual, missing_prob):\n",
    "            err = 1\n",
    "            if rec.QUAL > 0:\n",
    "                err = 1/rec.QUAL\n",
    "            o.write(\"%s\\n\" % \"locus %s pos %d %g\" % (rec.CHROM, rec.POS, err))\n",
    "            for sample in rec.samples:\n",
    "                ref = 0\n",
    "                alt = 0\n",
    "                if sample.data.RO:\n",
    "                    ref = sample.data.RO\n",
    "                if sample.data.AO:\n",
    "                    alt = sample.data.AO\n",
    "                try:\n",
    "                    o.write(\"%s\\n\" % \"%d %d\" % (ref, alt))\n",
    "                except:\n",
    "                    print rec.INFO\n",
    "                    print sample.data\n",
    "        if i % 5000 == 0:\n",
    "            print \"at %d\" % i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example:\n",
    "\n",
    "    $HOME/data7/src/afsource/af -g gypsy_freebayes_out.vcf_qual_20_missing_0.50.afsource -e 0.01 -n 1 -p gypsy_freebayes_out.vcf_qual_20_missing_0.50.afsource_e0.01_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "##Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hardy_file = \"/data7/eckertlab/gypsy_indiv/gypsy_freebayes_out.vcf.hwe\"\n",
    "het_file = \"/data7/eckertlab/gypsy_indiv/gypsy_freebayes_out.vcf.het\"\n",
    "freq_file = \"/data7/eckertlab/gypsy_indiv/gypsy_freebayes_out.vcf.frq\"\n",
    "counts_file = \"/data7/eckertlab/gypsy_indiv/gypsy_freebayes_out.vcf.frq.count\"\n",
    "z12_file = \"/data7/eckertlab/gypsy_indiv/gypsy_freebayes_out.vcf.012\"\n",
    "pos_file = \"/data7/eckertlab/gypsy_indiv/gypsy_freebayes_out.vcf.012.pos\"\n",
    "indv_file = \"/data7/eckertlab/gypsy_indiv/gypsy_freebayes_out.vcf.012.indv\"\n",
    "trans_table = \"translation_table.csv\"\n",
    "store_path = \"/data7/cfriedline/store_gypsy.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dview['store_path'] = store_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def put(path, obj):\n",
    "    s = pandas.HDFStore(store_path)\n",
    "    if path in s:\n",
    "        print \"updating %s\" % path\n",
    "        s.remove(path)\n",
    "    s[path] = obj\n",
    "    s.close()\n",
    "    \n",
    "def get(path):\n",
    "    s = pandas.HDFStore(store_path)\n",
    "    d = None\n",
    "    if path in s:\n",
    "        d = s[path]\n",
    "    s.close()\n",
    "    return d\n",
    "\n",
    "def in_store(path):\n",
    "    s = pandas.HDFStore(store_path)\n",
    "    val = False\n",
    "    if path in s:\n",
    "        val = True\n",
    "    s.close()\n",
    "    return val\n",
    "\n",
    "def remove(path):\n",
    "    s = pandas.HDFStore(store_path)\n",
    "    if path in s:\n",
    "        print \"removing %s\" % path\n",
    "        s.remove(path)\n",
    "    s.close()\n",
    "dview['get'] = get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Read in 012 file from vcf tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z12_df = None\n",
    "remove(\"z12_df\")\n",
    "if in_store('z12_df'):\n",
    "    z12_df = get('z12_df')\n",
    "else:\n",
    "    z12_df = pd.read_csv(z12_file, header=None, sep=\"\\t\")\n",
    "    z12_df = z12_df.drop(0, axis=1)\n",
    "    z12_df.columns = pd.Series(z12_df.columns)-1\n",
    "    put('z12_df', z12_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_allele_counts(series, filter_missing=True):\n",
    "    P = 0.0\n",
    "    Q = 0.0\n",
    "    PQ = 0.0\n",
    "    missing = 0.0\n",
    "    c = series.value_counts()\n",
    "    if 0 in c:\n",
    "        P = c[0]\n",
    "    if 2 in c:\n",
    "        Q = c[2]\n",
    "    if 1 in c:\n",
    "        PQ = c[1]\n",
    "    if -1 in c:\n",
    "        missing = c[-1]\n",
    "    missing_perc = missing/len(series)\n",
    "    if filter_missing and missing_perc > 0.5:\n",
    "#         print \"too much missing data %g\" % missing_perc\n",
    "        P = PQ = Q = 0.0\n",
    "    return (P, PQ, Q)\n",
    "\n",
    "def get_correction(n):\n",
    "    #for finite sample size\n",
    "    return (2*n)/(2*n-1)\n",
    "\n",
    "def get_allele_freqs(series, filter_missing=True):\n",
    "    c = get_allele_counts(series, filter_missing)\n",
    "    if c:\n",
    "        if sum(c) == 0:\n",
    "            return 0.0,0.0, 0.0, c\n",
    "        else:\n",
    "            P = 2*c[0] + c[1]\n",
    "            Q = 2*c[2] + c[1]\n",
    "            total = P + Q\n",
    "            p = P/total\n",
    "            q = Q/total\n",
    "            maf = min(p, q)\n",
    "            return maf, p, q, c \n",
    "        \n",
    "def get_he(series):\n",
    "    maf, p, q, c = get_allele_freqs(series)\n",
    "    if p:\n",
    "        He = 2 * p * q * get_correction(sum(c))\n",
    "        return He\n",
    "    return None\n",
    "\n",
    "def get_ho(series):\n",
    "    c = get_allele_counts(series)\n",
    "    if c:\n",
    "        if sum(c) == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            Ho = c[1]/(sum(c))\n",
    "            return Ho\n",
    "    return None\n",
    "    \n",
    "def get_Fis(df):\n",
    "#     df = df.ix[:,0:10]\n",
    "    #need cols b/c we might be loading modified table from h5, only want orig snp cols\n",
    "    cols = [x for x in df.columns if isinstance(x, numbers.Number)]\n",
    "    obs = df[cols].apply(get_ho)\n",
    "    exp = df[cols].apply(get_he)\n",
    "    Fis = 1 - obs/exp\n",
    "    return obs, exp, Fis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs = exp = Fis = None\n",
    "if in_store('Fis') > 0:\n",
    "    obs = get('obs')\n",
    "    exp = get('exp')\n",
    "    Fis = get('Fis')\n",
    "else:\n",
    "    obs, exp, Fis = get_Fis(z12_df)\n",
    "    put('obs',obs)\n",
    "    put('exp',exp)\n",
    "    put('Fis',Fis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Fis.replace([np.inf, -np.inf], np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(x, bins=12)\n",
    "plt.title(\"n=%d\" % len(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Remove based on F_is outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Fis_filter_fis = Fis[(Fis >= -0.5) & (Fis <= 0.5)] # by Fis value\n",
    "print \"removed\", (len(Fis)-len(Fis_filter_fis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    removed 19551"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Remove based on MAF < 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rare = []\n",
    "for snp_index in Fis_filter_fis.index:\n",
    "    maf, p, q, c = get_allele_freqs(z12_df[snp_index])\n",
    "    if maf < 0.01:\n",
    "        rare.append(snp_index)\n",
    "print len(rare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Fis_filter_fis_rare = Fis_filter_fis.drop(rare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Put filtered data back with sample/pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = None\n",
    "if in_store('df'):\n",
    "    df = get('df')\n",
    "    z12_df = get('z12_df')\n",
    "else:\n",
    "    trans_df = pd.read_csv(trans_table, header=0, index_col=0, sep=\"\\t\")\n",
    "    for i, line in enumerate(open(indv_file)):\n",
    "        line = line.strip()\n",
    "        trans = trans_df.ix[line]\n",
    "        z12_df.ix[i, 'sample'] = \"%s_%d\" % (trans['pop'], trans['indiv'])\n",
    "        z12_df.ix[i, 'pop'] = trans['pop']\n",
    "        z12_df.ix[i, 'dupl'] = trans['dup']\n",
    "    df = z12_df[z12_df['dupl']==0]\n",
    "    df = df.drop('dupl', axis=1)\n",
    "    put('df',df)\n",
    "    put('z12_df',z12_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pop_id = {}\n",
    "i = 0\n",
    "for p in sorted(z12_df['pop'].unique()):\n",
    "    pop_id[p] = i\n",
    "    i+=1\n",
    "pop_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hierf = None\n",
    "# store.remove('hierf')\n",
    "if in_store('hierf'):\n",
    "    hierf = get('hierf')\n",
    "    df = get('df')\n",
    "else:\n",
    "    hierf_trans = {0:11, 1:12, 2:22, -1:'NA'}\n",
    "    def apply_hierf_trans(series):\n",
    "        return [hierf_trans[x] if x in hierf_trans else x for x in series]\n",
    "    df = df.apply(apply_hierf_trans)\n",
    "\n",
    "    def assign_popid(series):\n",
    "        series['popid'] = pop_id[series['pop']]\n",
    "        return series\n",
    "    df = df.apply(assign_popid, axis=1)  \n",
    "\n",
    "    import numbers\n",
    "    hierfcols = ['sample', 'popid']\n",
    "    # hierfcols.extend(df.columns.values[0:-3])\n",
    "    hierfcols.extend(Fis_filter_fis_rare.index)\n",
    "    hierf = df[hierfcols].sort('popid')\n",
    "    cols = [None]*len(hierf.columns)\n",
    "    for i, x in enumerate(hierf.columns):\n",
    "        if isinstance(x, numbers.Number):\n",
    "            cols[i] = \"L%d\" % x\n",
    "        else:\n",
    "            cols[i] = x\n",
    "    hierf.columns = pd.Index(cols)\n",
    "    hierf.to_csv(\"hierfstat.txt\", header=True, index=False, sep=\"\\t\")\n",
    "    put('hierf',hierf)\n",
    "    put('df',df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(hierfstat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "data = read.table(\"hierfstat.txt\", header=T, sep=\"\\t\")\n",
    "levels = data.frame(data$popid)\n",
    "loci = data[,3:ncol(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "res = varcomp.glob(levels=levels, loci=loci, diploid=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "saveRDS(res, \"hierfstat.rds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "res = readRDS(\"hierfstat.rds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = com.convert_robj(robjects.r('res'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc_df = res['loc']\n",
    "F_df = res['F']\n",
    "overall_df = res['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_fst(series):\n",
    "    Hs = series[0]\n",
    "    Ht = sum(series)\n",
    "    return (Ht-Hs)/Ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loci_fst = loc_df.apply(compute_fst, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(loci_fst, bins=20)\n",
    "plt.title(\"n=%d (%.2f, %.2f)\" % (len(loci_fst), np.min(loci_fst), np.max(loci_fst)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "###Setup for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_data = None\n",
    "if in_store('pca_data'):\n",
    "    pca_data = get('pca_data')\n",
    "else:\n",
    "    filtered_cols = Fis_filter_fis_rare.index.tolist()\n",
    "    filtered_cols.extend(['sample', 'pop','dupl'])\n",
    "    pca_data = z12_df[filtered_cols]\n",
    "    pca_data = pca_data[pca_data['dupl']==0]\n",
    "    cols = [None]*len(pca_data.columns)\n",
    "    for i, x in enumerate(pca_data.columns):\n",
    "        if isinstance(x, numbers.Number):\n",
    "            cols[i] = \"L%d\" % x\n",
    "        else:\n",
    "            cols[i] = x\n",
    "    pca_data.columns = pd.Index(cols)\n",
    "    put('pca_data', pca_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def swap_alleles(series):\n",
    "#     series=pca_data[series]\n",
    "    if series.name.startswith(\"L\"):\n",
    "        maf,p,q,c = get_allele_freqs(series, filter_missing=False)\n",
    "        if maf == p:\n",
    "            return series.replace({0:2, 2:0})\n",
    "    return series\n",
    "\n",
    "def center_and_standardize_value(val, u, var):\n",
    "    if val == -1:\n",
    "        return 0.0\n",
    "    return (val-u)/var\n",
    "\n",
    "\n",
    "def center_and_standardize(series):\n",
    "#     series=pca_data[series]\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    if series.name.startswith(\"L\"):\n",
    "        maf,p,q,c = get_allele_freqs(series, filter_missing=False)\n",
    "        u = sum([i*x for i, x in enumerate(c)])/sum(c)\n",
    "        var = np.sqrt(maf*(1-maf))\n",
    "        return series.apply(center_and_standardize_value, args=(u, var))\n",
    "    return series\n",
    "\n",
    "# dview['swap_alleles'] = swap_alleles\n",
    "# dview['swap_allele'] = swap_allele\n",
    "# dview['center_and_standardize'] = center_and_standardize\n",
    "# dview['center_and_standardize_value'] = center_and_standardize_value\n",
    "# dview['get_allele_counts'] = get_allele_counts\n",
    "# dview['get_allele_freqs'] = get_allele_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dview.push({'pca_data':pca_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make sure minor allele is 2\n",
    "pca_maf = None\n",
    "if in_store('pca_maf'):\n",
    "    pca_maf = get('pca_maf')\n",
    "else:\n",
    "    pca_maf = pca_data.apply(swap_alleles)\n",
    "    # pca_maf = pd.concat(lview.map_async(swap_alleles, pca_data.columns.tolist()).get(), axis=1)\n",
    "    put('pca_maf',pca_maf)\n",
    "\n",
    "#standardize with minor allele freq\n",
    "pca_std = None\n",
    "pca_std_data = None\n",
    "if in_store('pca_std'):\n",
    "    pca_std = get('pca_std')\n",
    "    pca_std_data = get('pca_std_data')\n",
    "else:\n",
    "    pca_std = pca_maf.apply(center_and_standardize)\n",
    "    # pca_std = pd.concat(lview.map_async(center_and_standardize, [pca_maf[col] for col in pca_maf]).get(), axis=1)\n",
    "    #get only the data cols\n",
    "    pca_std_data = pca_std.ix[:,0:-3]\n",
    "    put('pca_std',pca_std)\n",
    "    put('pca_std_data',pca_std_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in xrange(5):\n",
    "    print pca_std_data.ix[:,x].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    count    1.890000e+02\n",
    "    mean     2.937098e-18\n",
    "    std      9.833783e-01\n",
    "    min     -3.631365e-01\n",
    "    25%     -3.631365e-01\n",
    "    50%      0.000000e+00\n",
    "    75%      0.000000e+00\n",
    "    max      5.326002e+00\n",
    "    dtype: float64\n",
    "    count    1.890000e+02\n",
    "    mean    -1.844498e-16\n",
    "    std      9.954059e-01\n",
    "    min     -6.188527e-01\n",
    "    25%     -6.188527e-01\n",
    "    50%      0.000000e+00\n",
    "    75%      0.000000e+00\n",
    "    max      2.922360e+00\n",
    "    dtype: float64\n",
    "    count    1.890000e+02\n",
    "    mean     3.289550e-17\n",
    "    std      1.302756e+00\n",
    "    min     -9.047280e-01\n",
    "    25%     -9.047280e-01\n",
    "    50%     -9.047280e-01\n",
    "    75%      1.758245e+00\n",
    "    max      4.421218e+00\n",
    "    dtype: float64\n",
    "    count    1.890000e+02\n",
    "    mean     4.346905e-17\n",
    "    std      1.379861e+00\n",
    "    min     -9.794946e-01\n",
    "    25%     -9.794946e-01\n",
    "    50%     -9.794946e-01\n",
    "    75%      1.552122e+00\n",
    "    max      4.083739e+00\n",
    "    dtype: float64\n",
    "    count    1.890000e+02\n",
    "    mean    -8.693810e-17\n",
    "    std      1.431318e+00\n",
    "    min     -1.326130e+00\n",
    "    25%     -1.326130e+00\n",
    "    50%      0.000000e+00\n",
    "    75%      8.450828e-01\n",
    "    max      3.016296e+00\n",
    "    dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = robjects.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prcomp = r('prcomp')\n",
    "summary = r('summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prcomp_res = prcomp(pca_std_data, scale=False, center=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print summary(prcomp_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = com.convert_robj(prcomp_res.rx2(\"x\"))\n",
    "x.index = pca_std_data.index\n",
    "joined = x.join(pca_maf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors=[pop_id[x.split(\"_\")[0]] for x in joined.sample.tolist()]\n",
    "labels = [x.split(\"_\")[0] for x in joined.sample.tolist()]\n",
    "plt.scatter(joined.PC1, joined.PC2, c=colors, s=50)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,8)\n",
    "plt.title(\"PCA of n=%d samples on %d loci\" % (len(joined), len(pca_std_data.columns)))\n",
    "plt.xlabel(\"PC1 (0.021%)\")\n",
    "plt.ylabel(\"PC2 (0.015%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_missing_perc(series):\n",
    "    missing = 0\n",
    "    total = 0\n",
    "    for x in series.index:\n",
    "        if x.startswith(\"L\"):\n",
    "            if series[x] == -1:\n",
    "                missing += 1\n",
    "            total +=1\n",
    "    return missing/total\n",
    "    \n",
    "missing_perc = joined.apply(compute_missing_perc, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined['missing_perc'] = missing_perc\n",
    "put('joined', joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(joined.missing_perc, joined.PC1)\n",
    "plt.xlabel(\"missing perc\")\n",
    "plt.ylabel(\"PC1\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://docs.google.com/uc?export=download&id=0B4xHxBFoPCoWT0NneHJadUI0OHM'\n",
    "response = urllib2.urlopen(url)\n",
    "pheno = pd.read_excel(response, \"Males-forGenomics-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pheno=pheno[['Population', 'Number', 'Mass', 'Pupual Duration', 'Total Dev Time']]\n",
    "for x in pheno.index:\n",
    "    pheno.ix[x, 'sample_pheno'] = \"%s_%d\" % (pheno.ix[x, 'Population'], pheno.ix[x, 'Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pheno_index = []\n",
    "missing = [] \n",
    "for x in pheno.index:\n",
    "    sample = pheno.ix[x]['sample_pheno']\n",
    "    target = joined[joined['sample']==sample]\n",
    "    if len(target.index):\n",
    "        i = target.index.tolist()[0]\n",
    "        pheno_index.append(i)\n",
    "    else:\n",
    "        missing.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pheno = pheno.drop(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pheno = pheno.set_index(pd.Index(pheno_index))\n",
    "pheno = pheno.sort()\n",
    "put('pheno', pheno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jp = None\n",
    "if in_store('jp'):\n",
    "    jp = get('jp')\n",
    "else:\n",
    "    jp=pd.concat([joined,pheno], axis=1)\n",
    "    jp=jp.drop(jp[np.isnan(jp['Mass'])].index)\n",
    "    put('jp', jp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "###PCA Price et al. 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "source('tw_calc.R')\n",
    "test=read.table(\"twtable\", header=F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_data2 = pca_data2_std = None\n",
    "if in_store('pca_data2'):\n",
    "    pca_data2 = get('pca_data2')\n",
    "    pca_data2_std = get('pca_data2_std')\n",
    "else:\n",
    "    pca_data2 = jp[[x for x in jp.columns if x.startswith(\"L\")]]\n",
    "    pca_data2_std = pca_data2.apply(center_and_standardize)\n",
    "    pca_data2_std.columns = [x.replace(\"L\", \"S\") for x in pca_data2_std.columns]\n",
    "    put('pca_data2',pca_data2)\n",
    "    put('pca_data2_std',pca_data2_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = robjects.r\n",
    "prcomp = r('prcomp')\n",
    "TWcalc = r('TWcalc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop = 0\n",
    "S0 = pca_data2_std.drop('S%d' % drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tw_e = tw_p = None\n",
    "if in_store('tw_p'):\n",
    "    tw_p = get('tw_p')\n",
    "    tw_e = get('tw_e')\n",
    "else:\n",
    "    tw = TWcalc(com.convert_to_r_matrix(S0), 10)\n",
    "    tw_p = com.convert_robj(tw.rx2(2))\n",
    "    tw_e = com.convert_robj(tw.rx2(1))\n",
    "    put('tw_p', pd.Series(tw_p))\n",
    "    put('tw_e', pd.Series(tw_e))\n",
    "print tw_e\n",
    "print tw_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tw_num = 0\n",
    "for i, p in enumerate(tw_p):\n",
    "    if p > 0.05:\n",
    "        tw_num = i\n",
    "        break\n",
    "print \"Tracy-Widom test yields %d axes of pop structure\" % tw_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_pca(df_name, col):\n",
    "    import traceback\n",
    "    try:\n",
    "        df = get(df_name)\n",
    "        df = df.drop(col, axis=1)\n",
    "        prcomp = rpy2.robjects.r('prcomp')\n",
    "        import pandas.rpy.common as com\n",
    "        res = prcomp(com.convert_to_r_matrix(df), center=False, scale=False)\n",
    "        return df_name, col, socket.gethostname(), com.convert_robj(res.rx2('x'))\n",
    "    except:\n",
    "        return df_name, col, socket.gethostname(), traceback.format_exc()\n",
    "dview['do_pca'] = do_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Do Leave-One-Out PCA for all loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rc, dview, lview = get_views()\n",
    "pca_jobs = []\n",
    "for i, col in enumerate(pca_data2_std):\n",
    "    if col.startswith(\"S\"):\n",
    "        if i % 1000 == 0:\n",
    "            print \"at %d\" % i\n",
    "#             do_pca('pca_data2_std', col)\n",
    "        pca_jobs.append(lview.apply_async(do_pca, 'pca_data2_std', col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write = True\n",
    "pca_ready = 0\n",
    "submitted = []\n",
    "completed = []\n",
    "pca_results = []\n",
    "failed = []\n",
    "for j in pca_jobs:\n",
    "#     print j.stdout\n",
    "    if j.ready():\n",
    "        pca_ready+=1\n",
    "        submitted.append(j.metadata.submitted)\n",
    "        completed.append(j.metadata.completed)\n",
    "        if write:\n",
    "            res = None\n",
    "            try:\n",
    "                res = j.get()\n",
    "                pca_results.append(res)\n",
    "                if not isinstance(res[-1], pd.DataFrame):\n",
    "                    print \"failed at %s\" % res\n",
    "                    failed.append(res)\n",
    "            except:\n",
    "                pass\n",
    "print \"%d/%d\" % (pca_ready, len(pca_jobs))\n",
    "print failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, p in enumerate(pca_results):\n",
    "    if i % 1000 == 0:\n",
    "        print i, p[0:3]\n",
    "    node = \"pca/%s\" % p[1]\n",
    "    put(node, p[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print max(completed)-min(submitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    0:29:16.933227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###group loci by maf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_maf_bin(series):\n",
    "    bins = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    maf, p, q, c = get_allele_freqs(pca_maf[series])\n",
    "    for elem in bins:\n",
    "        if maf <= elem:\n",
    "            return elem\n",
    "maf_bins = pca_maf.groupby(get_maf_bin, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loci = 0\n",
    "bin_names = []\n",
    "for b, pca_maf_bin in maf_bins:\n",
    "    bin_cols = [x for x in pca_maf_bin.columns if x.startswith(\"L\")]\n",
    "    std_cols = [x.replace(\"L\", \"S\") for x in bin_cols]\n",
    "    std_bin = pca_data2_std[std_cols]\n",
    "    print b, len(bin_cols), len(std_bin.columns)\n",
    "    loci += len(bin_cols)\n",
    "    bin_name = \"pca_bin/bin_%s/df\" % str(int(b*100))\n",
    "    put(bin_name, std_bin)    \n",
    "    bin_names.append(bin_name)\n",
    "print loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rc, dview, lview = get_views()\n",
    "bin_jobs = {}\n",
    "for bin_name in bin_names:\n",
    "    print bin_name\n",
    "    df = get(bin_name)\n",
    "    node_path = \"/\".join(bin_name.split(\"/\")[0:-1])\n",
    "    bin_jobs[node_path] = []\n",
    "    for i, col in enumerate(df):\n",
    "        if col.startswith(\"S\"):\n",
    "            if i % 1000 == 0:\n",
    "                print \"at %d\" % i\n",
    "#             print do_pca(bin_name, col)\n",
    "#             bin_jobs[node_path].append(col)\n",
    "            bin_jobs[node_path].append(lview.apply_async(do_pca, bin_name, col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write=True\n",
    "rerun_jobs = False\n",
    "failed_jobs = {}\n",
    "failed = []\n",
    "bin_results = []\n",
    "\n",
    "# if not store.is_open:\n",
    "#     store.open()\n",
    "for node_path, jobs in bin_jobs.items():\n",
    "    ready = 0\n",
    "    good = 0\n",
    "    local_failed = 0\n",
    "    \n",
    "    for j in jobs:\n",
    "        if j.ready():\n",
    "            res = j.get()\n",
    "            if isinstance(res[-1], pd.DataFrame):\n",
    "                good += 1\n",
    "                if write:                    \n",
    "                    bin_results.append(res)\n",
    "            else:\n",
    "                local_failed += 1\n",
    "                failed.append(res)\n",
    "            ready+=1\n",
    "    print \"%s: %d/%d (%d, %d)\" % (node_path, ready, len(jobs), good, local_failed)\n",
    "print \"failed %s\" % len(failed)\n",
    "# if rerun_jobs:\n",
    "#     failed_bins = set([x[0] for x in failed])\n",
    "#     failed_dict = {}\n",
    "#     for name in failed_bins:\n",
    "#         if not name in failed_dict:\n",
    "#             failed_dict[name] = []\n",
    "#     for f in failed:\n",
    "#         failed_dict[f[0]].append(f[1])\n",
    "#     for bin_name, cols in failed_dict.items():\n",
    "#         failed_jobs[bin_name] = []\n",
    "#         df = store[bin_name]\n",
    "#         node_path = \"/\".join(bin_name.split(\"/\")[0:-1])\n",
    "#         for col in cols:\n",
    "#             if col.startswith(\"S\"):\n",
    "# #                 print col\n",
    "#                 if i % 1000 == 0:\n",
    "#                     print \"at %d\" % i\n",
    "#     #             print do_pca(bin_name, col)\n",
    "#     #             bin_jobs[node_path].append(col)\n",
    "#                 failed_jobs[bin_name].append(lview.apply_async(do_pca, bin_name, col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pca_bin/bin_10: 4123/4123 (4123)\n",
    "    pca_bin/bin_20: 2988/2988 (2988)\n",
    "    pca_bin/bin_30: 2365/2365 (2365)\n",
    "    pca_bin/bin_40: 914/914 (914)\n",
    "    pca_bin/bin_50: 631/631 (631)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(bin_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for i, res in enumerate(bin_results):\n",
    "    parent = \"%s\" % \"/\".join(res[0].split(\"/\")[0:-1])\n",
    "    path = \"%s/%s\" % (\"/\".join(res[0].split(\"/\")[0:-1]), res[1])\n",
    "    \n",
    "    if not parent in counts:\n",
    "        counts[parent] = 1\n",
    "    else:\n",
    "        counts[parent] += 1\n",
    "        put(path,res[-1])\n",
    "    if i % 1000 == 0:\n",
    "        print i, path\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concat_data = pd.concat([get(x) for x in ['pca_data2_std', 'pca_data2', 'pheno']], \n",
    "                        axis=1, \n",
    "                        join=\"inner\")\n",
    "concat_data.columns = [x.replace(\" \", \"_\") for x in concat_data.columns]\n",
    "store['concat_data'] = concat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_fit_results(fit):\n",
    "    import numbers\n",
    "    import pandas as pd\n",
    "    summary = None\n",
    "    params = None\n",
    "    diag=None\n",
    "    curkey = None\n",
    "    for i, table in enumerate(fit.summary().tables):\n",
    "        if i != 1:\n",
    "            sdata = {}\n",
    "            for elem in table.data:\n",
    "                for ind in elem:\n",
    "                    ind = ind.strip()\n",
    "                    if \":\" in ind:\n",
    "                        curkey = ind\n",
    "                    else:\n",
    "                        val = None\n",
    "                        if isinstance(ind, numbers.Number):\n",
    "                            val = float(ind)\n",
    "                        else:\n",
    "                            val = ind\n",
    "                        sdata[curkey] = val\n",
    "            if i == 0:\n",
    "                summary = pd.Series(sdata)\n",
    "            else:\n",
    "                diag = pd.Series(sdata, dtype=float)\n",
    "        else:\n",
    "            params = pd.DataFrame(table.data)\n",
    "\n",
    "    params.ix[0,0] = \"x\"\n",
    "    params.columns=[x.replace(\" \", \"\") for x in params.ix[0,:]]\n",
    "    params = params.drop(0)\n",
    "    params.index = params.ix[:,0]\n",
    "    params.index.name = None\n",
    "    params=params.drop(\"x\", axis=1)\n",
    "    return summary, params, diag, fit.resid\n",
    "dview['convert_fit_results'] = convert_fit_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rc, dview, lview = get_views()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_model(df_path, locus, dep_var, tw_num):\n",
    "    import pandas as pd\n",
    "    import statsmodels.formula.api as smf\n",
    "    from scipy.stats.stats import pearsonr\n",
    "    import traceback\n",
    "    try:\n",
    "        store = pd.HDFStore(store_path)\n",
    "        df = store[df_path]\n",
    "        df.index = pd.Index([int(x) for x in df.index])\n",
    "        m = pd.concat([get('concat_data'), df], axis=1, join=\"inner\")\n",
    "        ind = \"+\".join([\"PC%d\" % (x+1) for x in xrange(tw_num)])\n",
    "        p_model = smf.ols(formula=\"%s ~ %s\" % (dep_var, ind), data=m)\n",
    "        p_fit = p_model.fit()\n",
    "        g_model = smf.ols(formula=\"%s ~ %s\" % (locus, ind), data=m)\n",
    "        g_fit = g_model.fit()\n",
    "        corrected_r = pearsonr(p_fit.resid, g_fit.resid)\n",
    "        uncorrected_r = pearsonr(m[dep_var], m[locus])\n",
    "        return {\"p_fit\":convert_fit_results(p_fit), \"g_fit\":convert_fit_results(g_fit), \"r_c\":corrected_r, \"r_u\":uncorrected_r}\n",
    "    except:\n",
    "        return traceback.format_exc()\n",
    "dview['fit_model']=fit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_node_children(node_name): \n",
    "    s = pd.HDFStore(store_path)\n",
    "    node = s.get_node(node_name)\n",
    "    children = []\n",
    "    for child, df in node._v_children.items():\n",
    "        children.append(df._v_pathname)\n",
    "    s.close()\n",
    "    return children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rc, dview, lview = get_views()\n",
    "# refresh=dview.apply(refresh_store).get()\n",
    "# print \"refreshed %d engines\" % len(refresh)\n",
    "rc, dview, lview = get_views()\n",
    "bin_model_jobs = {}\n",
    "dep_vars = [\"Mass\", \"Pupual_Duration\", \"Total_Dev_Time\"]\n",
    "if not store.is_open:\n",
    "    store.open()\n",
    "for node, children in bin_nodes.items():\n",
    "    node_count = 0\n",
    "    bin_model_jobs[node] = []\n",
    "    for df_path in children:\n",
    "        locus = df_path.split(\"/\")[-1]\n",
    "        if locus.startswith(\"S\"): #ignore the df node\n",
    "            node_count += 1\n",
    "            for dep_var in dep_vars:\n",
    "                bin_model_jobs[node].append(lview.apply_async(fit_model, df_path, locus, dep_var, tw_num))\n",
    "    print node, node_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin_model_results = {}\n",
    "for k, v in bin_model_jobs.items():\n",
    "    bin_model_results[k] = []\n",
    "    ready = 0\n",
    "    for j in v:\n",
    "        if j.ready():\n",
    "            ready+=1\n",
    "            res = j.get()\n",
    "            bin_model_results[k].append(res)\n",
    "    print \"%s: %d/%d\" % (k, ready, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summaries = {}\n",
    "diags = {}\n",
    "params = {}\n",
    "residuals = {}\n",
    "for k, results in bin_model_results.items():\n",
    "    for r in results:\n",
    "#         print r\n",
    "#         break\n",
    "        p_depvar = r['p_fit'][0]['Dep. Variable:']\n",
    "        g_depvar = r['g_fit'][0]['Dep. Variable:'] #g_depvar is also the locus that was excluded\n",
    "        \n",
    "        if not k in summaries:\n",
    "            summaries[k] = pd.DataFrame(index=r['p_fit'][0].keys())\n",
    "            diags[k] = pd.DataFrame(index=r['p_fit'][2].keys())\n",
    "            params[k] = {}\n",
    "            residuals[k] = pd.DataFrame()\n",
    "            \n",
    "        summaries[k][\"%s_p\"% g_depvar] = r['p_fit'][0]\n",
    "        summaries[k][\"%s_g\"% g_depvar] = r['g_fit'][0]\n",
    "        \n",
    "        params[k][\"%s_p\"% g_depvar] = r['p_fit'][1]\n",
    "        params[k][\"%s_g\"% g_depvar] = r['g_fit'][1]\n",
    "        \n",
    "        diags[k][\"%s_p\"% g_depvar] = r['p_fit'][2]\n",
    "        diags[k][\"%s_g\"% g_depvar] = r['g_fit'][2]\n",
    "        \n",
    "        residuals[k][\"%s_%s\" % (g_depvar, p_depvar)] = r['p_fit'][3]\n",
    "        residuals[k][\"%s_%s\" % (g_depvar, g_depvar)] = r['g_fit'][3]\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for bin_name, df in summaries.items():\n",
    "    path=\"model_fit/%s/summary\" % (bin_name)\n",
    "    print path\n",
    "    store[path] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for bin_name, df_dict in params.items():\n",
    "    path=\"model_fit/%s/params\" % (bin_name)\n",
    "    print path\n",
    "    store[path] = pd.Panel(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for bin_name, df in diags.items():\n",
    "    path=\"model_fit/%s/diags\" % (bin_name)\n",
    "    print path\n",
    "    store[path] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for bin_name, df in residuals.items():\n",
    "    path=\"model_fit/%s/residuals\" % (bin_name)\n",
    "    print path\n",
    "    store[path] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store['model_fit/pca_bin/bin_10/summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin_model_results = {}\n",
    "for k, v in bin_model_jobs.items():\n",
    "    bin_model_results[k] = []\n",
    "    ready = 0\n",
    "    for j in v:\n",
    "        if j.ready():\n",
    "            ready+=1\n",
    "            res = j.get()\n",
    "            bin_model_results[k].append(res)\n",
    "    print \"%s: %d/%d\" % (k, ready, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summaries = {}\n",
    "diags = {}\n",
    "params = {}\n",
    "residuals = {}\n",
    "r_c = {}\n",
    "r_u = {}\n",
    "\n",
    "for k, results in bin_model_results.items():\n",
    "    for i, r in enumerate(results):\n",
    "        if i % 1000 == 0:\n",
    "            print \"%s: at %d/%d\" % (k, i, len(results))\n",
    "#         print r\n",
    "#         break\n",
    "        p_depvar = r['p_fit'][0]['Dep. Variable:']\n",
    "        g_depvar = r['g_fit'][0]['Dep. Variable:'] #g_depvar is also the locus that was excluded\n",
    "        \n",
    "        if not k in summaries:\n",
    "            summaries[k] = pd.DataFrame(index=r['p_fit'][0].keys())\n",
    "            diags[k] = pd.DataFrame(index=r['p_fit'][2].keys())\n",
    "            params[k] = {}\n",
    "            residuals[k] = pd.DataFrame()\n",
    "            r_c[k] = pd.DataFrame()\n",
    "            r_u[k] = pd.DataFrame()\n",
    "            \n",
    "        summaries[k][\"%s_p\"% g_depvar] = r['p_fit'][0]\n",
    "        summaries[k][\"%s_g\"% g_depvar] = r['g_fit'][0]\n",
    "        \n",
    "        params[k][\"%s_p\"% g_depvar] = r['p_fit'][1]\n",
    "        params[k][\"%s_g\"% g_depvar] = r['g_fit'][1]\n",
    "        \n",
    "        diags[k][\"%s_p\"% g_depvar] = r['p_fit'][2]\n",
    "        diags[k][\"%s_g\"% g_depvar] = r['g_fit'][2]\n",
    "        \n",
    "        residuals[k][\"%s_%s\" % (g_depvar, p_depvar)] = r['p_fit'][3]\n",
    "        residuals[k][\"%s_%s\" % (g_depvar, g_depvar)] = r['g_fit'][3]\n",
    "        \n",
    "        r_c[k].ix[g_depvar, p_depvar] = r['r_c'][0]\n",
    "        r_u[k].ix[g_depvar, p_depvar] = r['r_u'][0]\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for bin_name, df in summaries.items():\n",
    "    path=\"model_fit/%s/summary\" % (bin_name)\n",
    "    print path\n",
    "    put(path,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for bin_name, df_dict in params.items():\n",
    "    path=\"model_fit/%s/params\" % (bin_name)\n",
    "    print path\n",
    "    put(path, pd.Panel(df_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for bin_name, df in diags.items():\n",
    "    path=\"model_fit/%s/diags\" % (bin_name)\n",
    "    print path\n",
    "    put(path, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for bin_name, df in residuals.items():\n",
    "    path=\"model_fit/%s/residuals\" % (bin_name)\n",
    "    print path\n",
    "    put(path, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for bin_name, df in r_c.items():\n",
    "    c_path=\"model_fit/%s/r_c\" % (bin_name)\n",
    "    u_path = \"model_fit/%s/r_u\" % (bin_name)\n",
    "    print c_path\n",
    "    put(c_path, df)\n",
    "    print u_path\n",
    "    put(u_path, r_u[bin_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print get('model_fit/pca_bin/bin_10/r_c').ix[0:5,:]\n",
    "print get('model_fit/pca_bin/bin_10/r_u').ix[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get('model_fit/pca_all/r_c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store['concat_data']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
